{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM poem generation for Shakespeare's sonnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get Shakespeare's poems from file\n",
    "def getPoems():\n",
    "    with open(\"./data/shakespeare.txt\", \"r\") as f:\n",
    "        data = f.read().lower()\n",
    "    # Split by poems\n",
    "    poems = data.split(\"\\n\\n\\n\")\n",
    "    # Remove 1st line of each poem\n",
    "    out = []\n",
    "    for poem in poems:\n",
    "        for i in range(len(poem)):\n",
    "            if poem[i]=='\\n':\n",
    "                break\n",
    "        out.append(poem[i+1:])\n",
    "    return out\n",
    "\n",
    "# Get character to integer dictionary for one hot encoding\n",
    "def getChardict(poems):\n",
    "    # merge all poems and get list of characters\n",
    "    data = \"\".join(poems)\n",
    "    # Get dictionary of characters for one hot encoding\n",
    "    chars = sorted(list(set(data)))\n",
    "    charint = dict((c, i) for i, c in enumerate(chars))\n",
    "    intchar = dict((i, c) for i, c in enumerate(chars))\n",
    "    return charint,intchar\n",
    "\n",
    "# Integer encode the poems\n",
    "def getIntPoems(charint,poems):\n",
    "    out = []\n",
    "    for poem in poems:\n",
    "        out.append([charint[char] for char in poem])\n",
    "    return np.array(out)\n",
    "\n",
    "def getWordlist():\n",
    "    with open(\"./data/shakespeare.txt\", \"r\") as f:\n",
    "        data = f.read().lower()\n",
    "    # Split by poems\n",
    "    poems = data.split(\"\\n\\n\\n\")\n",
    "    # Split poem by line, remove 1st line\n",
    "    poems = [poem.split(\"\\n\")[1:] for poem in poems]\n",
    "    # Split each line into a list of words\n",
    "    poems = [[line.split(\" \") for line in poem] for poem in poems]\n",
    "    # Strip punctuation : Optional\n",
    "    poems_by_lines = [[[word.strip(\",.:;?!()\").lower() for word in line] for line in poem ] for poem in poems]\n",
    "    # Combine all the lines in a single poem so that each pome is just a list of words\n",
    "    poems_by_words = [list(itertools.chain.from_iterable(poem)) for poem in poems_by_lines]\n",
    "    wordlist = np.concatenate(poems_by_words)\n",
    "    wordlist = np.unique(wordlist)\n",
    "    wordlist = np.sort(wordlist)\n",
    "    return wordlist\n",
    "\n",
    "# Get array of poems\n",
    "poems = getPoems()\n",
    "# Get integer encoding dictionary\n",
    "charint,intchar = getChardict(poems)\n",
    "# Get Integer encoded poem array\n",
    "IntPoems = getIntPoems(charint,poems)\n",
    "# Get sorted list of words from all shakespeare poems\n",
    "wordlist=np.array(getWordlist()[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17652, 40, 38)\n",
      "(17652, 38)\n"
     ]
    }
   ],
   "source": [
    "# Generate X and Y training sets from each poem\n",
    "def getCharacters(poem,n=40,skip=10):\n",
    "    Xtrain = [poem[i:i+n] for i in range(0,len(poem)-n,skip)]\n",
    "    Ytrain = [poem[i+n] for i in range(0,len(poem)-n,skip)]\n",
    "    return Xtrain,Ytrain\n",
    "\n",
    "# Generate training data \n",
    "Ntime = 80\n",
    "skip = 5\n",
    "Xtrain = []\n",
    "Ytrain = []\n",
    "for poem in IntPoems:\n",
    "    Xt,Yt = getCharacters(poem,Ntime,skip)\n",
    "    Xtrain.append(Xt)\n",
    "    Ytrain.append(Yt)\n",
    "\n",
    "Ytrain = np.concatenate(Ytrain)\n",
    "Xtrain = np.concatenate(Xtrain)\n",
    "\n",
    "# One hot encode the training vectors\n",
    "import keras\n",
    "Yt = keras.utils.np_utils.to_categorical(Ytrain)\n",
    "Xt = keras.utils.np_utils.to_categorical(Xtrain)\n",
    "\n",
    "print(Xt.shape)\n",
    "print(Yt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_8 (LSTM)                (None, 200)               191200    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 38)                7638      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 38)                0         \n",
      "=================================================================\n",
      "Total params: 198,838\n",
      "Trainable params: 198,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Dense, Activation, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "Nchars = len(charint)\n",
    "Ntime = 40\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, input_shape=(Xt.shape[1],Xt.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Nchars))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 3.0351\n",
      "Epoch 00001: loss improved from inf to 3.03423, saving model to ./data/Data_LSTM-01-3.0342.hdf5\n",
      "17652/17652 [==============================] - 31s 2ms/step - loss: 3.0342\n",
      "Epoch 2/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.7835\n",
      "Epoch 00002: loss improved from 3.03423 to 2.78176, saving model to ./data/Data_LSTM-02-2.7818.hdf5\n",
      "17652/17652 [==============================] - 30s 2ms/step - loss: 2.7818\n",
      "Epoch 3/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.4772\n",
      "Epoch 00003: loss improved from 2.78176 to 2.47586, saving model to ./data/Data_LSTM-03-2.4759.hdf5\n",
      "17652/17652 [==============================] - 30s 2ms/step - loss: 2.4759\n",
      "Epoch 4/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.3403\n",
      "Epoch 00004: loss improved from 2.47586 to 2.33861, saving model to ./data/Data_LSTM-04-2.3386.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 2.3386\n",
      "Epoch 5/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.2615\n",
      "Epoch 00005: loss improved from 2.33861 to 2.26043, saving model to ./data/Data_LSTM-05-2.2604.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 2.2604\n",
      "Epoch 6/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.2017\n",
      "Epoch 00006: loss improved from 2.26043 to 2.20275, saving model to ./data/Data_LSTM-06-2.2027.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 2.2027\n",
      "Epoch 7/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.1517\n",
      "Epoch 00007: loss improved from 2.20275 to 2.15045, saving model to ./data/Data_LSTM-07-2.1505.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 2.1505\n",
      "Epoch 8/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.1062\n",
      "Epoch 00008: loss improved from 2.15045 to 2.10606, saving model to ./data/Data_LSTM-08-2.1061.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 2.1061\n",
      "Epoch 9/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.0575\n",
      "Epoch 00009: loss improved from 2.10606 to 2.05821, saving model to ./data/Data_LSTM-09-2.0582.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 2.0582\n",
      "Epoch 10/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.0108\n",
      "Epoch 00010: loss improved from 2.05821 to 2.01079, saving model to ./data/Data_LSTM-10-2.0108.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 2.0108\n",
      "Epoch 11/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.9722\n",
      "Epoch 00011: loss improved from 2.01079 to 1.97183, saving model to ./data/Data_LSTM-11-1.9718.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 1.9718\n",
      "Epoch 12/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.9256\n",
      "Epoch 00012: loss improved from 1.97183 to 1.92709, saving model to ./data/Data_LSTM-12-1.9271.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 1.9271\n",
      "Epoch 13/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.8924\n",
      "Epoch 00013: loss improved from 1.92709 to 1.89259, saving model to ./data/Data_LSTM-13-1.8926.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 1.8926\n",
      "Epoch 14/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.8646\n",
      "Epoch 00014: loss improved from 1.89259 to 1.86377, saving model to ./data/Data_LSTM-14-1.8638.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 1.8638\n",
      "Epoch 15/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.8229\n",
      "Epoch 00015: loss improved from 1.86377 to 1.82345, saving model to ./data/Data_LSTM-15-1.8234.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 1.8234\n",
      "Epoch 16/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.7858\n",
      "Epoch 00016: loss improved from 1.82345 to 1.78675, saving model to ./data/Data_LSTM-16-1.7868.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.7868\n",
      "Epoch 17/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.7489\n",
      "Epoch 00017: loss improved from 1.78675 to 1.74807, saving model to ./data/Data_LSTM-17-1.7481.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.7481\n",
      "Epoch 18/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.7125\n",
      "Epoch 00018: loss improved from 1.74807 to 1.71178, saving model to ./data/Data_LSTM-18-1.7118.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.7118\n",
      "Epoch 19/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.6742\n",
      "Epoch 00019: loss improved from 1.71178 to 1.67289, saving model to ./data/Data_LSTM-19-1.6729.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.6729\n",
      "Epoch 20/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.6275\n",
      "Epoch 00020: loss improved from 1.67289 to 1.62721, saving model to ./data/Data_LSTM-20-1.6272.hdf5\n",
      "17652/17652 [==============================] - 29s 2ms/step - loss: 1.6272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x135ce3eb8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "fname=\"./data/Data_LSTM2-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(fname, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(Xt, Yt, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate poems from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CharToInt(charint,text):\n",
    "    return np.array([charint[char] for char in text])\n",
    "    \n",
    "def IntToChar(intchar,text):\n",
    "    return \"\".join([intchar[char] for char in text])\n",
    "\n",
    "# helper function to sample an index from a probability array\n",
    "def sample(a, temperature=1.0):\n",
    "    A = np.log(a) / temperature\n",
    "    A = np.exp(A)\n",
    "    A = A/np.sum(A)\n",
    "    A = A/np.sum(A)\n",
    "    return np.argmax(np.random.multinomial(1, A))\n",
    "\n",
    "# If char is letter in word list\n",
    "def IsWordLetter(char):\n",
    "    letters = '\\'abcdefghijklmnopqrstuvwxyz'\n",
    "    # If the current letter is in word list, then return 1\n",
    "    for i in letters:\n",
    "        if char==i:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def validWords(text,wordlist,charint,intchar):\n",
    "    # Valid letters in wordlist\n",
    "    letters = '\\'abcdefghijklmnopqrstuvwxyz'\n",
    "    strtext = IntToChar(intchar,text)\n",
    "    # If the current letter is not in word list, then all character options are valid\n",
    "    if IsWordLetter(strtext[-1])==False:\n",
    "        return 1\n",
    "    \n",
    "    # Find current word we are building\n",
    "    for i in range(1,200):\n",
    "        curword = strtext[len(strtext)-i:]\n",
    "        if IsWordLetter(curword[0])==False:\n",
    "            curword = curword[1:]\n",
    "            break\n",
    "    # Find if current word is in list of words\n",
    "    L = []\n",
    "    for word in wordlist:\n",
    "        x = word.find(curword)\n",
    "        if x==0:\n",
    "            if len(curword) < len(word):\n",
    "                L.append(word[len(curword)])\n",
    "\n",
    "    # If no words in dictionary are found then the word must end\n",
    "    if L==[]:\n",
    "        L = [' ','\\n',':',';']\n",
    "    L=np.unique(L)\n",
    "    # Build output vector\n",
    "    out = np.zeros(len(charint))\n",
    "    for i in L:\n",
    "        out[charint[i]]=1.0\n",
    "    return out\n",
    "\n",
    "def WordSample(text, wordlist, charint,intchar, a, temperature=1.0):\n",
    "    A = np.log(a) / temperature\n",
    "    A = np.exp(A)\n",
    "    b = validWords(text,wordlist,charint,intchar)\n",
    "    A = np.multiply(A,b)\n",
    "    # Normalize\n",
    "    A = A/np.sum(A)\n",
    "    return np.argmax(np.random.multinomial(1, A))\n",
    "\n",
    "def generatePoem(model,intchar,charint,seed,temp=1.0):\n",
    "    print('Seed = ',seed)\n",
    "    IntSeed = CharToInt(charint,seed)\n",
    "    IntOut = IntSeed\n",
    "    temp = 1.0\n",
    "    lines = 13\n",
    "    # generate characters\n",
    "    for i in range(1000):\n",
    "        X = IntOut[i:i+Ntime]\n",
    "        OneHot_X = keras.utils.np_utils.to_categorical([X],num_classes=len(charint))\n",
    "        Ypred = model.predict(OneHot_X)\n",
    "        idx = sample(Ypred[0],temp)\n",
    "        IntOut = np.concatenate((IntOut,[idx]))\n",
    "        # Count number of poem lines generated\n",
    "        if idx==0:\n",
    "            lines-=1\n",
    "        if lines==0:\n",
    "            break\n",
    "    return IntToChar(intchar,IntOut)\n",
    "\n",
    "# Generate poem constraining to real words\n",
    "def generatePoem2(model,wordlist,intchar,charint,seed,temp=1.0):\n",
    "    print('Seed = ',seed)\n",
    "    IntSeed = CharToInt(charint,seed)\n",
    "    IntOut = IntSeed\n",
    "    temp = 1.0\n",
    "    lines = 13\n",
    "    # generate characters\n",
    "    for i in range(1000):\n",
    "        X = IntOut[i:i+Ntime]\n",
    "        OneHot_X = keras.utils.np_utils.to_categorical([X],num_classes=len(charint))\n",
    "        Ypred = model.predict(OneHot_X)\n",
    "        idx = WordSample(IntOut,wordlist,charint,intchar,Ypred[0],temp)\n",
    "        IntOut = np.concatenate((IntOut,[idx]))\n",
    "        # Count number of poem lines generated\n",
    "        if idx==0:\n",
    "            lines-=1\n",
    "        if lines==0:\n",
    "            break\n",
    "    return IntToChar(intchar,IntOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poem at temp =  1.5 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "geeditly thing domi lelt dising thiths and those her.\n",
      ", and gife can the pariefild hor hought?\n",
      "not no ir bling nhcheerwhor her thin that sen' blowsttlenst gresed:\n",
      "and with thee having not, and thie sthith,\n",
      "and thy selmorns i kno, thei shiesung)?\n",
      "for om whise thee sermy not io that seprecuse.\n",
      "  you or  ond deatey taltiit memy fill-graingl?\n",
      "mure worf i sbeet in this sipas fon tele,\n",
      "for nothols not ound of the and leadsy hedweat.\n",
      "n be to tay fomerend your shillk, and thy weae apy will,\n",
      "unwirnigh    arnmengiget in ach,\n",
      "all pooutir's on fon nowr chich my e,un yers\n",
      "shringe since erenfuigh lioks now of thy ight be.\n",
      "\n",
      "Generated Poem at temp =  0.75 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "loth prussing of and of sich dovighn\n",
      " sall stalle prace him nithe tham that pevesuwer she,\n",
      " urt ruth the poodst pross caving my stall,\n",
      "nore and by allikn sfof-faited like of secend,\n",
      "y to pightine hem whome richle knost;\n",
      "no hiod is galenwing and-tof in owereged,\n",
      "the sulling umont thperisy mase is ounmy,\n",
      "madtinot houll ene, inkbes cy till's past.\n",
      "  the angste souct now the prome, nt my heart'se conte.\n",
      "  and blins epe im ill-stoul dast and ailly kewercengnows?\n",
      "which poarhin  what conetere a thome,\n",
      "thal swemenee hin ingsry um steeles lids,\n",
      "for  ou sreren nothe i hat's bespranie,\n",
      "\n",
      "Generated Poem at temp =  0.25 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "akeains boullik bl he me their in ney,\n",
      "for so in hate (belcksand ho hase the cairs,\n",
      "to mikn me the wurl if to nome a cavell seea,\n",
      "the inds lovy shatherse thilg in of wert-lecked.\n",
      "  while ingeotr hit neald in theen desuss for craiks,\n",
      " hf in, bet yer of iinsne, live you sesk and to my hess alloke,\n",
      "to ming with wither have you sund,\n",
      "that hes shee ti seelsc-notmend no gerseevire.\n",
      "by that wieg beas of hel bits my hell,\n",
      "tay and hor alles orw in tile edond ut ave yout\n",
      "to pay'sr sake, ourtise ferrele.\n",
      "  for whle beinss be mpershe habd ave love,\n",
      " heaving nem polming havt or me wonksing,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "temp = [1.5,0.75,0.25]\n",
    "for i in temp:\n",
    "    print('Generated Poem at temp = ',i,':')\n",
    "    print(generatePoem(model,intchar,charint,seed,temp=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poem at temp =  1.5 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "edge wouldst youth's greater under messengers:\n",
      "which chose sweet-bearer it share took four sullen sue sheaves\n",
      "sheaves often my here's best,\n",
      "by sullen inconstant often twain livery falls these world-dead cheeks rise eyed attainted:\n",
      "audit lusty forlorn wolf thee aright:\n",
      "but thence merits clay therein hours 'had hated verses:\n",
      "and things decayed if fist that's more deeds:\n",
      "approve votary thousand doom took history peace:\n",
      "issueless issueless rearward hours poesy:\n",
      "and simple sums orient roses blow selling nearly days\n",
      "th' instant onwards forth termed messengers;\n",
      "our indeed sheaves if one eased and imperfect:\n",
      "chief forty semblance hill went one (poorly thee more nearly\n",
      "\n",
      "Generated Poem at temp =  0.75 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "shallowest my wills  fame ne'er-converted if years wert;eyed issueless:\n",
      "entertain ne'er-beated if hill attending tongues imitated:\n",
      "but therein effectually heed ruth records\n",
      "and that's fire owes it issueless graces seat\n",
      "offenders often were't\n",
      "thyself tombed touches offenders stole tanned:\n",
      "allege outright theirs white unworthiness:\n",
      "livery allege th' edge:uncertain themes instinct others' tillage were't:\n",
      "merits lover's something short-hounds it which trouble denote thee surety-bereft intelligence sins commence:\n",
      "chief it derive promise that's hearts which truly miser my falls:\n",
      " hands lover's anchored appears issueless incertainties:\n",
      "nor bettered merits hated nothing thither yours offenders\n",
      "and things bastard society thoughts doubting loud:\n",
      "\n",
      "Generated Poem at temp =  0.25 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "whence buried nothing nothing steepy yet offender's by withering mars winds incapable\n",
      "my semblance hands open semblance orphans cupid dyed\n",
      "and bears finds thence peep sheaves verses\n",
      "ornaments shallowest once issueless verses key\n",
      "showers waiting crave painting sickness indeed:\n",
      "given youngly thinks vermilion by monarch's allege betraying messengers\n",
      "our tallies offenders fist theirs issueless mended\n",
      "and granting were't feathered into resemble ink brings\n",
      "former aid nothing things death's forced:\n",
      "down-teeming thyself bred ah were't bears trenches dignity:\n",
      "nor kills bestow'st both neglected steals\n",
      "now that's inconstant issueless dost shines double-pyramids nights\n",
      "whence older well-were't   pale plight fairer and but:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "temp = [1.5,0.75,0.25]\n",
    "for i in temp:\n",
    "    print('Generated Poem at temp = ',i,':')\n",
    "    print(generatePoem2(model,wordlist,intchar,charint,seed,temp=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 40, 200)           191200    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 40, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 38)                7638      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 38)                0         \n",
      "=================================================================\n",
      "Total params: 519,638\n",
      "Trainable params: 519,638\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2 layer LSTM\n",
    "Nchars = len(charint)\n",
    "Ntime = 40\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(200, input_shape=(Xt.shape[1],Xt.shape[2]),return_sequences=True))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(LSTM(200))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(Nchars))\n",
    "model2.add(Activation('softmax'))\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 3.0345\n",
      "Epoch 00001: loss improved from inf to 3.03414, saving model to ./data/Data_LSTM-01-3.0341.hdf5\n",
      "17652/17652 [==============================] - 89s 5ms/step - loss: 3.0341\n",
      "Epoch 2/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.8090\n",
      "Epoch 00002: loss improved from 3.03414 to 2.80788, saving model to ./data/Data_LSTM-02-2.8079.hdf5\n",
      "17652/17652 [==============================] - 72s 4ms/step - loss: 2.8079\n",
      "Epoch 3/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.4449\n",
      "Epoch 00003: loss improved from 2.80788 to 2.44372, saving model to ./data/Data_LSTM-03-2.4437.hdf5\n",
      "17652/17652 [==============================] - 76s 4ms/step - loss: 2.4437\n",
      "Epoch 4/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.2815\n",
      "Epoch 00004: loss improved from 2.44372 to 2.28120, saving model to ./data/Data_LSTM-04-2.2812.hdf5\n",
      "17652/17652 [==============================] - 70s 4ms/step - loss: 2.2812\n",
      "Epoch 5/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.1668\n",
      "Epoch 00005: loss improved from 2.28120 to 2.16701, saving model to ./data/Data_LSTM-05-2.1670.hdf5\n",
      "17652/17652 [==============================] - 70s 4ms/step - loss: 2.1670\n",
      "Epoch 6/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.0829\n",
      "Epoch 00006: loss improved from 2.16701 to 2.08202, saving model to ./data/Data_LSTM-06-2.0820.hdf5\n",
      "17652/17652 [==============================] - 70s 4ms/step - loss: 2.0820\n",
      "Epoch 7/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.0051\n",
      "Epoch 00007: loss improved from 2.08202 to 2.00487, saving model to ./data/Data_LSTM-07-2.0049.hdf5\n",
      "17652/17652 [==============================] - 70s 4ms/step - loss: 2.0049\n",
      "Epoch 8/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.9413\n",
      "Epoch 00008: loss improved from 2.00487 to 1.94232, saving model to ./data/Data_LSTM-08-1.9423.hdf5\n",
      "17652/17652 [==============================] - 69s 4ms/step - loss: 1.9423\n",
      "Epoch 9/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.8848\n",
      "Epoch 00009: loss improved from 1.94232 to 1.88593, saving model to ./data/Data_LSTM-09-1.8859.hdf5\n",
      "17652/17652 [==============================] - 69s 4ms/step - loss: 1.8859\n",
      "Epoch 10/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.8414\n",
      "Epoch 00010: loss improved from 1.88593 to 1.84099, saving model to ./data/Data_LSTM-10-1.8410.hdf5\n",
      "17652/17652 [==============================] - 69s 4ms/step - loss: 1.8410\n",
      "Epoch 11/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.7877\n",
      "Epoch 00011: loss improved from 1.84099 to 1.78784, saving model to ./data/Data_LSTM-11-1.7878.hdf5\n",
      "17652/17652 [==============================] - 69s 4ms/step - loss: 1.7878\n",
      "Epoch 12/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.7456\n",
      "Epoch 00012: loss improved from 1.78784 to 1.74627, saving model to ./data/Data_LSTM-12-1.7463.hdf5\n",
      "17652/17652 [==============================] - 69s 4ms/step - loss: 1.7463\n",
      "Epoch 13/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.6929\n",
      "Epoch 00013: loss improved from 1.74627 to 1.69267, saving model to ./data/Data_LSTM-13-1.6927.hdf5\n",
      "17652/17652 [==============================] - 69s 4ms/step - loss: 1.6927\n",
      "Epoch 14/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.6512\n",
      "Epoch 00014: loss improved from 1.69267 to 1.65086, saving model to ./data/Data_LSTM-14-1.6509.hdf5\n",
      "17652/17652 [==============================] - 69s 4ms/step - loss: 1.6509\n",
      "Epoch 15/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.5957\n",
      "Epoch 00015: loss improved from 1.65086 to 1.59568, saving model to ./data/Data_LSTM-15-1.5957.hdf5\n",
      "17652/17652 [==============================] - 69s 4ms/step - loss: 1.5957\n",
      "Epoch 16/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.5469\n",
      "Epoch 00016: loss improved from 1.59568 to 1.54597, saving model to ./data/Data_LSTM-16-1.5460.hdf5\n",
      "17652/17652 [==============================] - 74s 4ms/step - loss: 1.5460\n",
      "Epoch 17/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.4886\n",
      "Epoch 00017: loss improved from 1.54597 to 1.48979, saving model to ./data/Data_LSTM-17-1.4898.hdf5\n",
      "17652/17652 [==============================] - 78s 4ms/step - loss: 1.4898\n",
      "Epoch 18/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.4313\n",
      "Epoch 00018: loss improved from 1.48979 to 1.43014, saving model to ./data/Data_LSTM-18-1.4301.hdf5\n",
      "17652/17652 [==============================] - 83s 5ms/step - loss: 1.4301\n",
      "Epoch 19/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.3786\n",
      "Epoch 00019: loss improved from 1.43014 to 1.37908, saving model to ./data/Data_LSTM-19-1.3791.hdf5\n",
      "17652/17652 [==============================] - 80s 5ms/step - loss: 1.3791\n",
      "Epoch 20/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.3205\n",
      "Epoch 00020: loss improved from 1.37908 to 1.31987, saving model to ./data/Data_LSTM-20-1.3199.hdf5\n",
      "17652/17652 [==============================] - 82s 5ms/step - loss: 1.3199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x131f42be0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "fname=\"./data/Data_LSTM2-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(fname, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model2.fit(Xt, Yt, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poem at temp =  1.5 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "u hack ene rund it befaired ouncengid,\n",
      "thy beauty madk, and that shoust bestase make wnow,\n",
      "which my searh's and vord no tellsey by fale,\n",
      "  for ain with thy lovert's lose anther so love cride:\n",
      "thou though i speeirs told their whisk chave rromomed,\n",
      "put i tave remamy thou brsadest whem dooust,\n",
      "'ush faving freit leave had tithe's fay thee more sele:\n",
      "th'sefo gralice you bestory blought wat,\n",
      "that som am thy dzadt, and that with this prowed deathen griead;\n",
      "to the tan the somledzst so eir's with (bvave,\n",
      "parist lakind him lover of mance incantube,\n",
      "sifingut now, and moring well buind on ath's dowe.\n",
      "by that reif this love thy porst to ostnele.\n",
      "\n",
      "Generated Poem at temp =  0.75 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "the may the frait doth the vourso of (ids.\n",
      "the sweet farteringing to he porce are:\n",
      "a simven's tel afd beadt pzoth by to de.\n",
      "  thene, the wraild qriesecmed for their sweet thy self hed,\n",
      "tere so war is blooks both in moth ard mort,\n",
      "is all thes resgoud ik my frient of thin,\n",
      "and somle strengse and of hiver enceas reatte?\n",
      "  far this presed blingues not that waenth sweet,\n",
      "fzich it thoughts gadt welzerevez thou whils-nor the dinged then thee,\n",
      "face shous to dey for fulling those it so sae,\n",
      "for this stne af and praise, which my hast,\n",
      "or laks this parss of lolk to theight hreanime,\n",
      "whene shank grote to ousterise be sower dowithe:\n",
      "\n",
      "Generated Poem at temp =  0.25 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "when the will have doth seef this had.\n",
      "  then so it pratisul fart is but of this, 'ang,\n",
      "astheres made thzse boust condeoch dot sand,\n",
      "i may's zom to theee i see breathed tave qute,\n",
      "toze mase whatuip that gyout dasfich pors:\n",
      "cond is felseing of nzel thingst tome alane.\n",
      "eave youn nimbort ad my his tome doth hiss,\n",
      "  the esead wallh som their in thy lank wzill,\n",
      "and that fall sumpen cate my gont, being,\n",
      "and those bese mlange heapt thze bess thee,\n",
      "nom nothing train detime, and haf my selow,\n",
      "to hich thee will thanz's in my net, revirs,\n",
      "of goct desilith add love, and gunt love un\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "temp = [1.5,0.75,0.25]\n",
    "for i in temp:\n",
    "    print('Generated Poem at temp = ',i,':')\n",
    "    print(generatePoem(model2,intchar,charint,seed,temp=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poem at temp =  1.5 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "thence shamed black mine eyes' bloody shaken\n",
      "astonished my robe sang once crooked groans:\n",
      "cunning are and my more destroys one:\n",
      "one angry withering allow now steeled vilest\n",
      "gates fairest gives although says forsworn being days:\n",
      "correspondence send'st fairer fairest farther hindmost tells bears\n",
      "  forwards merits beauty's doom feathered bower head\n",
      "often theirs often tombed is't looks down-betraying tattered:\n",
      "and my fearing amazeth silvered potions potions dully\n",
      "methods march times nothing mountain nativity farther something:\n",
      "thence that's after-torn mayst once worthiness amends blessed-cheap thine intents roses\n",
      "doth incapable offices puts mine awards\n",
      "rainy from thee bitterness often took twenty heart's painter's died:\n",
      "\n",
      "Generated Poem at temp =  0.75 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "that's my lusty ensconce issueless tombed thee gives deemed and dead thee:\n",
      "songs: astronomy truths mightst kings dost lovers' torment keeps\n",
      "touches wert (under inconstant days crystal\n",
      "gay chaste makes withal question outright thee reigned:\n",
      "ornaments often eyes' wanting touches divert:\n",
      "now worship theirs times hath love's betraying:\n",
      "that's things dead bearing head theirs elements:\n",
      "tombed issueless more cries offenders joy\n",
      "turned nothing dates that's makes offender's list bearing:\n",
      "yet speechless keeps which it nothing journey\n",
      "gates threescore bettered pursuing this days\n",
      "whate'er thence thee weeks tongues if still:\n",
      "thence black thousand plain messengers;\n",
      "\n",
      "Generated Poem at temp =  0.25 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "that's mended touches shun my gardens makes finds\n",
      "but thyself tombed these thyself my languished known seeking grown:\n",
      "news inward wires one former pilgrimage restored\n",
      "outstripped news aye graces asked thyself knew\n",
      "and praised fairer himself withering onwards runs brow\n",
      "whether something surfeit former since mow sooner tombed:\n",
      "astronomy respects offender's mortality praised:\n",
      "bettered well-my fuel therein ornaments\n",
      "ornaments praised thee owners best,\n",
      "by level years mother's cold issueless times\n",
      "thoughts wouldst thanks likeness truths about destroys room made:\n",
      "whatsoever sweets big touches children's horses\n",
      "beds' incertainties astonished cancelled bearing:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "temp = [1.5,0.75,0.25]\n",
    "for i in temp:\n",
    "    print('Generated Poem at temp = ',i,':')\n",
    "    print(generatePoem2(model2,wordlist,intchar,charint,seed,temp=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
