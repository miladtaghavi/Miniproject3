{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM poem generation for Shakespeare's sonnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get Shakespeare's poems from file\n",
    "def getPoems():\n",
    "    with open(\"./data/shakespeare.txt\", \"r\") as f:\n",
    "        data = f.read().lower()\n",
    "    # Split by poems\n",
    "    poems = data.split(\"\\n\\n\\n\")\n",
    "    # Remove 1st line of each poem\n",
    "    out = []\n",
    "    for poem in poems:\n",
    "        for i in range(len(poem)):\n",
    "            if poem[i]=='\\n':\n",
    "                break\n",
    "        out.append(poem[i+1:])\n",
    "    return out\n",
    "\n",
    "# Get character to integer dictionary for one hot encoding\n",
    "def getChardict(poems):\n",
    "    # merge all poems and get list of characters\n",
    "    data = \"\".join(poems)\n",
    "    # Get dictionary of characters for one hot encoding\n",
    "    chars = sorted(list(set(data)))\n",
    "    charint = dict((c, i) for i, c in enumerate(chars))\n",
    "    intchar = dict((i, c) for i, c in enumerate(chars))\n",
    "    return charint,intchar\n",
    "\n",
    "# Integer encode the poems\n",
    "def getIntPoems(charint,poems):\n",
    "    out = []\n",
    "    for poem in poems:\n",
    "        out.append([charint[char] for char in poem])\n",
    "    return np.array(out)\n",
    "\n",
    "# Get array of poems\n",
    "poems = getPoems()\n",
    "# Get integer encoding dictionary\n",
    "charint,intchar = getChardict(poems)\n",
    "# Get Integer encoded poem array\n",
    "IntPoems = getIntPoems(charint,poems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17652, 40, 38)\n",
      "(17652, 38)\n"
     ]
    }
   ],
   "source": [
    "# Generate X and Y training sets from each poem\n",
    "def getCharacters(poem,n=40,skip=10):\n",
    "    Xtrain = [poem[i:i+n] for i in range(0,len(poem)-n,skip)]\n",
    "    Ytrain = [poem[i+n] for i in range(0,len(poem)-n,skip)]\n",
    "    return Xtrain,Ytrain\n",
    "\n",
    "# Generate training data \n",
    "Ntime = 40\n",
    "skip = 5\n",
    "Xtrain = []\n",
    "Ytrain = []\n",
    "for poem in IntPoems:\n",
    "    Xt,Yt = getCharacters(poem,Ntime,skip)\n",
    "    Xtrain.append(Xt)\n",
    "    Ytrain.append(Yt)\n",
    "\n",
    "Ytrain = np.concatenate(Ytrain)\n",
    "Xtrain = np.concatenate(Xtrain)\n",
    "\n",
    "# One hot encode the training vectors\n",
    "import keras\n",
    "Yt = keras.utils.np_utils.to_categorical(Ytrain)\n",
    "Xt = keras.utils.np_utils.to_categorical(Xtrain)\n",
    "\n",
    "print(Xt.shape)\n",
    "print(Yt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_8 (LSTM)                (None, 200)               191200    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 38)                7638      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 38)                0         \n",
      "=================================================================\n",
      "Total params: 198,838\n",
      "Trainable params: 198,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Dense, Activation, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "Nchars = len(charint)\n",
    "Ntime = 40\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, input_shape=(Xt.shape[1],Xt.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Nchars))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 3.0351\n",
      "Epoch 00001: loss improved from inf to 3.03423, saving model to ./data/Data_LSTM-01-3.0342.hdf5\n",
      "17652/17652 [==============================] - 31s 2ms/step - loss: 3.0342\n",
      "Epoch 2/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.7835\n",
      "Epoch 00002: loss improved from 3.03423 to 2.78176, saving model to ./data/Data_LSTM-02-2.7818.hdf5\n",
      "17652/17652 [==============================] - 30s 2ms/step - loss: 2.7818\n",
      "Epoch 3/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.4772\n",
      "Epoch 00003: loss improved from 2.78176 to 2.47586, saving model to ./data/Data_LSTM-03-2.4759.hdf5\n",
      "17652/17652 [==============================] - 30s 2ms/step - loss: 2.4759\n",
      "Epoch 4/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.3403\n",
      "Epoch 00004: loss improved from 2.47586 to 2.33861, saving model to ./data/Data_LSTM-04-2.3386.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 2.3386\n",
      "Epoch 5/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.2615\n",
      "Epoch 00005: loss improved from 2.33861 to 2.26043, saving model to ./data/Data_LSTM-05-2.2604.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 2.2604\n",
      "Epoch 6/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.2017\n",
      "Epoch 00006: loss improved from 2.26043 to 2.20275, saving model to ./data/Data_LSTM-06-2.2027.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 2.2027\n",
      "Epoch 7/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.1517\n",
      "Epoch 00007: loss improved from 2.20275 to 2.15045, saving model to ./data/Data_LSTM-07-2.1505.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 2.1505\n",
      "Epoch 8/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.1062\n",
      "Epoch 00008: loss improved from 2.15045 to 2.10606, saving model to ./data/Data_LSTM-08-2.1061.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 2.1061\n",
      "Epoch 9/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.0575\n",
      "Epoch 00009: loss improved from 2.10606 to 2.05821, saving model to ./data/Data_LSTM-09-2.0582.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 2.0582\n",
      "Epoch 10/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.0108\n",
      "Epoch 00010: loss improved from 2.05821 to 2.01079, saving model to ./data/Data_LSTM-10-2.0108.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 2.0108\n",
      "Epoch 11/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.9722\n",
      "Epoch 00011: loss improved from 2.01079 to 1.97183, saving model to ./data/Data_LSTM-11-1.9718.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 1.9718\n",
      "Epoch 12/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.9256\n",
      "Epoch 00012: loss improved from 1.97183 to 1.92709, saving model to ./data/Data_LSTM-12-1.9271.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 1.9271\n",
      "Epoch 13/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.8924\n",
      "Epoch 00013: loss improved from 1.92709 to 1.89259, saving model to ./data/Data_LSTM-13-1.8926.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 1.8926\n",
      "Epoch 14/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.8646\n",
      "Epoch 00014: loss improved from 1.89259 to 1.86377, saving model to ./data/Data_LSTM-14-1.8638.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 1.8638\n",
      "Epoch 15/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.8229\n",
      "Epoch 00015: loss improved from 1.86377 to 1.82345, saving model to ./data/Data_LSTM-15-1.8234.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 1.8234\n",
      "Epoch 16/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.7858\n",
      "Epoch 00016: loss improved from 1.82345 to 1.78675, saving model to ./data/Data_LSTM-16-1.7868.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.7868\n",
      "Epoch 17/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.7489\n",
      "Epoch 00017: loss improved from 1.78675 to 1.74807, saving model to ./data/Data_LSTM-17-1.7481.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.7481\n",
      "Epoch 18/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.7125\n",
      "Epoch 00018: loss improved from 1.74807 to 1.71178, saving model to ./data/Data_LSTM-18-1.7118.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.7118\n",
      "Epoch 19/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.6742\n",
      "Epoch 00019: loss improved from 1.71178 to 1.67289, saving model to ./data/Data_LSTM-19-1.6729.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.6729\n",
      "Epoch 20/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.6275\n",
      "Epoch 00020: loss improved from 1.67289 to 1.62721, saving model to ./data/Data_LSTM-20-1.6272.hdf5\n",
      "17652/17652 [==============================] - 29s 2ms/step - loss: 1.6272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x135ce3eb8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "fname=\"./data/Data_LSTM-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(fname, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(Xt, Yt, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate poems from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CharToInt(charint,text):\n",
    "    return np.array([charint[char] for char in text])\n",
    "    \n",
    "def IntToChar(intchar,text):\n",
    "    return \"\".join([intchar[char] for char in text])\n",
    "\n",
    "# helper function to sample an index from a probability array\n",
    "def sample(a, temperature=1.0):\n",
    "    a = np.log(a) / temperature\n",
    "    a = np.exp(a)\n",
    "    a = a/np.sum(a)*.99\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))\n",
    "\n",
    "def generatePoem(model,intchar,charint,seed,temp=1.0):\n",
    "    print('Seed = ',seed)\n",
    "    IntSeed = CharToInt(charint,seed)\n",
    "    IntOut = IntSeed\n",
    "    temp = 1.0\n",
    "    lines = 13\n",
    "    # generate characters\n",
    "    for i in range(1000):\n",
    "        X = IntOut[i:i+Ntime]\n",
    "        OneHot_X = keras.utils.np_utils.to_categorical([X],num_classes=len(charint))\n",
    "        Ypred = model.predict(OneHot_X)\n",
    "        idx = sample(Ypred[0],temp)\n",
    "        IntOut = np.concatenate((IntOut,[idx]))\n",
    "        # Count number of poem lines generated\n",
    "        if idx==0:\n",
    "            lines-=1\n",
    "        if lines==0:\n",
    "            break\n",
    "    return IntToChar(intchar,IntOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poem at temp =  1.5 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "shawe sheise solf will i wairs, and tore yem will youre bots meesige's bnot\n",
      "s tuld grikeads my pinguwid, likes ah suet-oligh?\n",
      "sabins hes topering to have is blof no,\n",
      "with the  heazwer which ip yaimy faor wire,\n",
      "migite't thouling's of sunele theesstugh bewhereste's loed,\n",
      "the lige to stelis shaise mis 't thee enerye-.\n",
      "orhel his szeghe woald whis more, but thenjpeind,\n",
      "far i lose is ase,\n",
      "to (tine on titsee onome cquetseaties wall's and ornjlagh,\n",
      "which thi hy this pruse-unt my heartzing.\n",
      "aillan ke tinze pootd;, and with hil feis my dears enms.\n",
      "rr willds brtate as nce breasing lazken thene, zillting dantew\n",
      " ithan, yourest thoul in dout seze not culd,\n",
      "\n",
      "Generated Poem at temp =  0.75 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "so diszely wheich no tongue houachd asist past\n",
      "reselo as wore, uf thut sime ot the cossmed,\n",
      "but love is ured the fasis gictareis bores\n",
      "worleds of mine, and hat caike thet thou ald an arthecusequte:\n",
      "is misered ceneded store watos me aling,\n",
      "werich il more then aften thy nene,\n",
      "in inglase me mereed it beath gzklem.\n",
      "thou in my helstrone leasu, hith thy swill,\n",
      "thin ecorein hou dest has  eiver she wrowe.,\n",
      "thy heary me benuse singened; as alle's lzve oth toldst canks mumen,\n",
      "the mald a m berst hish so ley, witi sublly eres.\n",
      "bitaning sw wies hot mell rase azlons mowezs,\n",
      "your shat ichin 's mza'e the hellig cenath,\n",
      "\n",
      "Generated Poem at temp =  0.25 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "th t mize freamen psens, mest brait an's pnot to digizsweet,\n",
      "rake it ly thus spollingz)lfith the wornd.\n",
      "f and gronind hatmyze ao dazeem foine.\n",
      "  whenzeg it be raceof ham oresey misedsty:\n",
      "the eresabe ul heizer sweef ealthoring.\n",
      " ansen to me's for toze spease, and roving thene someld.\n",
      "thut whaich thoue ir hath relf in be thyes prowe.\n",
      "n the ai sin, alenqeourssupe vighss ort pris,\n",
      "a loves by thie piver shalfed beanh treen,\n",
      "  o  has ceillor hive disting tu mheze porst,\n",
      "thou eo's alows ne ir priess beavy on prceiege.\n",
      "  frine not ving'z whincone, of is su:\n",
      "c my secet so cange, il is be fiat,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "temp = [1.5,0.75,0.25]\n",
    "for i in temp:\n",
    "    print('Generated Poem at temp = ',i,':')\n",
    "    print(generatePoem(model,intchar,charint,seed,temp=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 40, 200)           191200    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 40, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 38)                7638      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 38)                0         \n",
      "=================================================================\n",
      "Total params: 519,638\n",
      "Trainable params: 519,638\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2 layer LSTM\n",
    "Nchars = len(charint)\n",
    "Ntime = 40\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(200, input_shape=(Xt.shape[1],Xt.shape[2]),return_sequences=True))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(LSTM(200))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(Nchars))\n",
    "model2.add(Activation('softmax'))\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 3.0345\n",
      "Epoch 00001: loss improved from inf to 3.03414, saving model to ./data/Data_LSTM-01-3.0341.hdf5\n",
      "17652/17652 [==============================] - 89s 5ms/step - loss: 3.0341\n",
      "Epoch 2/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.8090\n",
      "Epoch 00002: loss improved from 3.03414 to 2.80788, saving model to ./data/Data_LSTM-02-2.8079.hdf5\n",
      "17652/17652 [==============================] - 72s 4ms/step - loss: 2.8079\n",
      "Epoch 3/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.4449\n",
      "Epoch 00003: loss improved from 2.80788 to 2.44372, saving model to ./data/Data_LSTM-03-2.4437.hdf5\n",
      "17652/17652 [==============================] - 76s 4ms/step - loss: 2.4437\n",
      "Epoch 4/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.2815\n",
      "Epoch 00004: loss improved from 2.44372 to 2.28120, saving model to ./data/Data_LSTM-04-2.2812.hdf5\n",
      "17652/17652 [==============================] - 70s 4ms/step - loss: 2.2812\n",
      "Epoch 5/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.1668\n",
      "Epoch 00005: loss improved from 2.28120 to 2.16701, saving model to ./data/Data_LSTM-05-2.1670.hdf5\n",
      "17652/17652 [==============================] - 70s 4ms/step - loss: 2.1670\n",
      "Epoch 6/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.0829\n",
      "Epoch 00006: loss improved from 2.16701 to 2.08202, saving model to ./data/Data_LSTM-06-2.0820.hdf5\n",
      "17652/17652 [==============================] - 70s 4ms/step - loss: 2.0820\n",
      "Epoch 7/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.0051\n",
      "Epoch 00007: loss improved from 2.08202 to 2.00487, saving model to ./data/Data_LSTM-07-2.0049.hdf5\n",
      "17652/17652 [==============================] - 70s 4ms/step - loss: 2.0049\n",
      "Epoch 8/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.9413\n",
      "Epoch 00008: loss improved from 2.00487 to 1.94232, saving model to ./data/Data_LSTM-08-1.9423.hdf5\n",
      "17652/17652 [==============================] - 69s 4ms/step - loss: 1.9423\n",
      "Epoch 9/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.8848\n",
      "Epoch 00009: loss improved from 1.94232 to 1.88593, saving model to ./data/Data_LSTM-09-1.8859.hdf5\n",
      "17652/17652 [==============================] - 69s 4ms/step - loss: 1.8859\n",
      "Epoch 10/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.8414\n",
      "Epoch 00010: loss improved from 1.88593 to 1.84099, saving model to ./data/Data_LSTM-10-1.8410.hdf5\n",
      "17652/17652 [==============================] - 69s 4ms/step - loss: 1.8410\n",
      "Epoch 11/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.7877\n",
      "Epoch 00011: loss improved from 1.84099 to 1.78784, saving model to ./data/Data_LSTM-11-1.7878.hdf5\n",
      "17652/17652 [==============================] - 69s 4ms/step - loss: 1.7878\n",
      "Epoch 12/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.7456\n",
      "Epoch 00012: loss improved from 1.78784 to 1.74627, saving model to ./data/Data_LSTM-12-1.7463.hdf5\n",
      "17652/17652 [==============================] - 69s 4ms/step - loss: 1.7463\n",
      "Epoch 13/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.6929\n",
      "Epoch 00013: loss improved from 1.74627 to 1.69267, saving model to ./data/Data_LSTM-13-1.6927.hdf5\n",
      "17652/17652 [==============================] - 69s 4ms/step - loss: 1.6927\n",
      "Epoch 14/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.6512\n",
      "Epoch 00014: loss improved from 1.69267 to 1.65086, saving model to ./data/Data_LSTM-14-1.6509.hdf5\n",
      "17652/17652 [==============================] - 69s 4ms/step - loss: 1.6509\n",
      "Epoch 15/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.5957\n",
      "Epoch 00015: loss improved from 1.65086 to 1.59568, saving model to ./data/Data_LSTM-15-1.5957.hdf5\n",
      "17652/17652 [==============================] - 69s 4ms/step - loss: 1.5957\n",
      "Epoch 16/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.5469\n",
      "Epoch 00016: loss improved from 1.59568 to 1.54597, saving model to ./data/Data_LSTM-16-1.5460.hdf5\n",
      "17652/17652 [==============================] - 74s 4ms/step - loss: 1.5460\n",
      "Epoch 17/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.4886\n",
      "Epoch 00017: loss improved from 1.54597 to 1.48979, saving model to ./data/Data_LSTM-17-1.4898.hdf5\n",
      "17652/17652 [==============================] - 78s 4ms/step - loss: 1.4898\n",
      "Epoch 18/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.4313\n",
      "Epoch 00018: loss improved from 1.48979 to 1.43014, saving model to ./data/Data_LSTM-18-1.4301.hdf5\n",
      "17652/17652 [==============================] - 83s 5ms/step - loss: 1.4301\n",
      "Epoch 19/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.3786\n",
      "Epoch 00019: loss improved from 1.43014 to 1.37908, saving model to ./data/Data_LSTM-19-1.3791.hdf5\n",
      "17652/17652 [==============================] - 80s 5ms/step - loss: 1.3791\n",
      "Epoch 20/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.3205\n",
      "Epoch 00020: loss improved from 1.37908 to 1.31987, saving model to ./data/Data_LSTM-20-1.3199.hdf5\n",
      "17652/17652 [==============================] - 82s 5ms/step - loss: 1.3199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x131f42be0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "fname=\"./data/Data_LSTM-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(fname, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model2.fit(Xt, Yt, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poem at temp =  1.5 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "who preceseeme u tail of that hails in me,\n",
      "  that so? that mory that mose secpounts\n",
      "af indcents time's now ond to thee,\n",
      "then shact aok that faart of thin his was,\n",
      "  hather that to the smy self forded gat\n",
      "nof bent's bestrast the dotpst self dost eace,\n",
      "far theme to beared, my cay weres for tibes\n",
      "the pansty cond made hon my helf:\n",
      "  mine will ow will itrnef-kning no oonsurejdss).\n",
      "  far loeve portereds to sick hem to time,\n",
      "the priscode, with doreavy astirnsobe mind (eved.\n",
      "when forsing facn gentlyzy meary trull to stzen,\n",
      "the forncummoth and the werest of mand,\n",
      "\n",
      "Generated Poem at temp =  0.75 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "the bratos so zatce, reomer forc my grie.\n",
      "  (nos worte thee broos zos though tight self pain:\n",
      "theling, the mousty beauty so,\n",
      "the viine arath with poves of till swyet.\n",
      "fere thou will's ppay this pense thze sum fair,\n",
      "their zemiter love: sepstaiged not me,\n",
      "whele thy gorte, by in, bind nagh andzeresh best.\n",
      "so in toly swart-is buts of enest cooss.\n",
      "  that geath fack besemushed and of told\n",
      "and by sumf inde with seld seed,\n",
      "to sulf the world thy digpoad thut for mindlath,\n",
      "that nothoug in mate i the sweep cothough to love roch drads.\n",
      "  thougzt to mincentis of thy forshed deack,\n",
      "\n",
      "Generated Poem at temp =  0.25 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "then on the mendred of men wat doth sofment's,\n",
      "asthanbthe bestwar my sun, incenst,\n",
      "anzto it love thee, the wornd fave loven,\n",
      "centhes i stourroved shath voth to the wries.\n",
      "   thene chell you are incheart thee radzing,\n",
      "on theiss te-wist, and zustound of priece.\n",
      "the formmy dey fict love's fase thy stweng gos,\n",
      "and thene vimbs nom ind the ant shall brewais,\n",
      "not zim that is ligh not hat sheet beare,\n",
      "but the beauty of my delfet sifend.\n",
      "that more thes uffed i ty self arith deepsest buts,\n",
      "and thou suld grict, and moraant of nor,\n",
      "whene to made a crierer, wath? bland bith mazt,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "temp = [1.5,0.75,0.25]\n",
    "for i in temp:\n",
    "    print('Generated Poem at temp = ',i,':')\n",
    "    print(generatePoem(model2,intchar,charint,seed,temp=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
