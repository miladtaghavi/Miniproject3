{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM poem generation for Shakespeare's sonnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get Shakespeare's poems from file\n",
    "def getPoems():\n",
    "    with open(\"./data/shakespeare.txt\", \"r\") as f:\n",
    "        data = f.read().lower()\n",
    "    # Split by poems\n",
    "    poems = data.split(\"\\n\\n\\n\")\n",
    "    # Remove 1st line of each poem\n",
    "    out = []\n",
    "    for poem in poems:\n",
    "        for i in range(len(poem)):\n",
    "            if poem[i]=='\\n':\n",
    "                break\n",
    "        out.append(poem[i+1:])\n",
    "    return out\n",
    "\n",
    "# Get character to integer dictionary for one hot encoding\n",
    "def getChardict(poems):\n",
    "    # merge all poems and get list of characters\n",
    "    data = \"\".join(poems)\n",
    "    # Get dictionary of characters for one hot encoding\n",
    "    chars = sorted(list(set(data)))\n",
    "    charint = dict((c, i) for i, c in enumerate(chars))\n",
    "    intchar = dict((i, c) for i, c in enumerate(chars))\n",
    "    return charint,intchar\n",
    "\n",
    "# Integer encode the poems\n",
    "def getIntPoems(charint,poems):\n",
    "    out = []\n",
    "    for poem in poems:\n",
    "        out.append([charint[char] for char in poem])\n",
    "    return np.array(out)\n",
    "\n",
    "def getWordlist():\n",
    "    with open(\"./data/shakespeare.txt\", \"r\") as f:\n",
    "        data = f.read().lower()\n",
    "    # Split by poems\n",
    "    poems = data.split(\"\\n\\n\\n\")\n",
    "    # Split poem by line, remove 1st line\n",
    "    poems = [poem.split(\"\\n\")[1:] for poem in poems]\n",
    "    # Split each line into a list of words\n",
    "    poems = [[line.split(\" \") for line in poem] for poem in poems]\n",
    "    # Strip punctuation : Optional\n",
    "    poems_by_lines = [[[word.strip(\",.:;?!()\").lower() for word in line] for line in poem ] for poem in poems]\n",
    "    # Combine all the lines in a single poem so that each pome is just a list of words\n",
    "    poems_by_words = [list(itertools.chain.from_iterable(poem)) for poem in poems_by_lines]\n",
    "    wordlist = np.concatenate(poems_by_words)\n",
    "    wordlist = np.unique(wordlist)\n",
    "    wordlist = np.sort(wordlist)\n",
    "    return wordlist\n",
    "\n",
    "# Get array of poems\n",
    "poems = getPoems()\n",
    "# Get integer encoding dictionary\n",
    "charint,intchar = getChardict(poems)\n",
    "# Get Integer encoded poem array\n",
    "IntPoems = getIntPoems(charint,poems)\n",
    "# Get sorted list of words from all shakespeare poems\n",
    "wordlist=np.array(getWordlist()[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17652, 40, 38)\n",
      "(17652, 38)\n"
     ]
    }
   ],
   "source": [
    "# Generate X and Y training sets from each poem\n",
    "def getCharacters(poem,n=40,skip=10):\n",
    "    Xtrain = [poem[i:i+n] for i in range(0,len(poem)-n,skip)]\n",
    "    Ytrain = [poem[i+n] for i in range(0,len(poem)-n,skip)]\n",
    "    return Xtrain,Ytrain\n",
    "\n",
    "# Generate training data \n",
    "Ntime = 40\n",
    "skip = 5\n",
    "Xtrain = []\n",
    "Ytrain = []\n",
    "for poem in IntPoems:\n",
    "    Xt,Yt = getCharacters(poem,Ntime,skip)\n",
    "    Xtrain.append(Xt)\n",
    "    Ytrain.append(Yt)\n",
    "\n",
    "Ytrain = np.concatenate(Ytrain)\n",
    "Xtrain = np.concatenate(Xtrain)\n",
    "\n",
    "# One hot encode the training vectors\n",
    "import keras\n",
    "Yt = keras.utils.np_utils.to_categorical(Ytrain)\n",
    "Xt = keras.utils.np_utils.to_categorical(Xtrain)\n",
    "\n",
    "print(Xt.shape)\n",
    "print(Yt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 200)               191200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 38)                7638      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 38)                0         \n",
      "=================================================================\n",
      "Total params: 198,838\n",
      "Trainable params: 198,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Dense, Activation, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "Nchars = len(charint)\n",
    "Ntime = 40\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, input_shape=(Xt.shape[1],Xt.shape[2])))\n",
    "model.add(Dropout(0.0))\n",
    "model.add(Dense(Nchars))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.8101\n",
      "Epoch 00001: loss improved from inf to 1.80976, saving model to ./data/Data_LSTM1layer_100-01-1.8098.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.8098\n",
      "Epoch 2/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.7684\n",
      "Epoch 00002: loss improved from 1.80976 to 1.76835, saving model to ./data/Data_LSTM1layer_100-02-1.7684.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.7684\n",
      "Epoch 3/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.7256\n",
      "Epoch 00003: loss improved from 1.76835 to 1.72537, saving model to ./data/Data_LSTM1layer_100-03-1.7254.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.7254\n",
      "Epoch 4/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.6828\n",
      "Epoch 00004: loss improved from 1.72537 to 1.68369, saving model to ./data/Data_LSTM1layer_100-04-1.6837.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.6837\n",
      "Epoch 5/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.6495\n",
      "Epoch 00005: loss improved from 1.68369 to 1.65060, saving model to ./data/Data_LSTM1layer_100-05-1.6506.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 1.6506\n",
      "Epoch 6/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.5921\n",
      "Epoch 00006: loss improved from 1.65060 to 1.59198, saving model to ./data/Data_LSTM1layer_100-06-1.5920.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 1.5920\n",
      "Epoch 7/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.5416\n",
      "Epoch 00007: loss improved from 1.59198 to 1.54159, saving model to ./data/Data_LSTM1layer_100-07-1.5416.hdf5\n",
      "17652/17652 [==============================] - 31s 2ms/step - loss: 1.5416\n",
      "Epoch 8/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.4867\n",
      "Epoch 00008: loss improved from 1.54159 to 1.48859, saving model to ./data/Data_LSTM1layer_100-08-1.4886.hdf5\n",
      "17652/17652 [==============================] - 30s 2ms/step - loss: 1.4886\n",
      "Epoch 9/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.4261\n",
      "Epoch 00009: loss improved from 1.48859 to 1.42612, saving model to ./data/Data_LSTM1layer_100-09-1.4261.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 1.4261\n",
      "Epoch 10/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.3681\n",
      "Epoch 00010: loss improved from 1.42612 to 1.36762, saving model to ./data/Data_LSTM1layer_100-10-1.3676.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 1.3676\n",
      "Epoch 11/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.3016\n",
      "Epoch 00011: loss improved from 1.36762 to 1.30268, saving model to ./data/Data_LSTM1layer_100-11-1.3027.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 1.3027\n",
      "Epoch 12/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.2288\n",
      "Epoch 00012: loss improved from 1.30268 to 1.22975, saving model to ./data/Data_LSTM1layer_100-12-1.2297.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 1.2297\n",
      "Epoch 13/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.1523\n",
      "Epoch 00013: loss improved from 1.22975 to 1.15353, saving model to ./data/Data_LSTM1layer_100-13-1.1535.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 1.1535\n",
      "Epoch 14/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.0827\n",
      "Epoch 00014: loss improved from 1.15353 to 1.08259, saving model to ./data/Data_LSTM1layer_100-14-1.0826.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 1.0826\n",
      "Epoch 15/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.9999\n",
      "Epoch 00015: loss improved from 1.08259 to 1.00027, saving model to ./data/Data_LSTM1layer_100-15-1.0003.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 1.0003\n",
      "Epoch 16/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.9257\n",
      "Epoch 00016: loss improved from 1.00027 to 0.92628, saving model to ./data/Data_LSTM1layer_100-16-0.9263.hdf5\n",
      "17652/17652 [==============================] - 34s 2ms/step - loss: 0.9263\n",
      "Epoch 17/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.8396\n",
      "Epoch 00017: loss improved from 0.92628 to 0.84075, saving model to ./data/Data_LSTM1layer_100-17-0.8408.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 0.8408\n",
      "Epoch 18/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.7647\n",
      "Epoch 00018: loss improved from 0.84075 to 0.76449, saving model to ./data/Data_LSTM1layer_100-18-0.7645.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 0.7645\n",
      "Epoch 19/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.6964\n",
      "Epoch 00019: loss improved from 0.76449 to 0.69661, saving model to ./data/Data_LSTM1layer_100-19-0.6966.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 0.6966\n",
      "Epoch 20/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.6145\n",
      "Epoch 00020: loss improved from 0.69661 to 0.61502, saving model to ./data/Data_LSTM1layer_100-20-0.6150.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 0.6150\n",
      "Epoch 21/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.5595\n",
      "Epoch 00021: loss improved from 0.61502 to 0.55963, saving model to ./data/Data_LSTM1layer_100-21-0.5596.hdf5\n",
      "17652/17652 [==============================] - 34s 2ms/step - loss: 0.5596\n",
      "Epoch 22/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.4890\n",
      "Epoch 00022: loss improved from 0.55963 to 0.49039, saving model to ./data/Data_LSTM1layer_100-22-0.4904.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 0.4904\n",
      "Epoch 23/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.4306\n",
      "Epoch 00023: loss improved from 0.49039 to 0.43107, saving model to ./data/Data_LSTM1layer_100-23-0.4311.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 0.4311\n",
      "Epoch 24/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.3735\n",
      "Epoch 00024: loss improved from 0.43107 to 0.37367, saving model to ./data/Data_LSTM1layer_100-24-0.3737.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 0.3737\n",
      "Epoch 25/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.3324\n",
      "Epoch 00025: loss improved from 0.37367 to 0.33285, saving model to ./data/Data_LSTM1layer_100-25-0.3328.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 0.3328\n",
      "Epoch 26/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.2887\n",
      "Epoch 00026: loss improved from 0.33285 to 0.28876, saving model to ./data/Data_LSTM1layer_100-26-0.2888.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 0.2888\n",
      "Epoch 27/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.2597\n",
      "Epoch 00027: loss improved from 0.28876 to 0.25987, saving model to ./data/Data_LSTM1layer_100-27-0.2599.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 0.2599\n",
      "Epoch 28/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.2267\n",
      "Epoch 00028: loss improved from 0.25987 to 0.22714, saving model to ./data/Data_LSTM1layer_100-28-0.2271.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 0.2271\n",
      "Epoch 29/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.1878\n",
      "Epoch 00029: loss improved from 0.22714 to 0.18758, saving model to ./data/Data_LSTM1layer_100-29-0.1876.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 0.1876\n",
      "Epoch 30/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.1551\n",
      "Epoch 00030: loss improved from 0.18758 to 0.15534, saving model to ./data/Data_LSTM1layer_100-30-0.1553.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 0.1553\n",
      "Epoch 31/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.1370\n",
      "Epoch 00031: loss improved from 0.15534 to 0.13699, saving model to ./data/Data_LSTM1layer_100-31-0.1370.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 0.1370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.1184\n",
      "Epoch 00032: loss improved from 0.13699 to 0.11845, saving model to ./data/Data_LSTM1layer_100-32-0.1185.hdf5\n",
      "17652/17652 [==============================] - 34s 2ms/step - loss: 0.1185\n",
      "Epoch 33/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.1065\n",
      "Epoch 00033: loss improved from 0.11845 to 0.10679, saving model to ./data/Data_LSTM1layer_100-33-0.1068.hdf5\n",
      "17652/17652 [==============================] - 34s 2ms/step - loss: 0.1068\n",
      "Epoch 34/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.1163\n",
      "Epoch 00034: loss did not improve\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 0.1163\n",
      "Epoch 35/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.1222\n",
      "Epoch 00035: loss did not improve\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 0.1223\n",
      "Epoch 36/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.1269\n",
      "Epoch 00036: loss did not improve\n",
      "17652/17652 [==============================] - 34s 2ms/step - loss: 0.1273\n",
      "Epoch 37/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.1064\n",
      "Epoch 00037: loss improved from 0.10679 to 0.10667, saving model to ./data/Data_LSTM1layer_100-37-0.1067.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 0.1067\n",
      "Epoch 38/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0856\n",
      "Epoch 00038: loss improved from 0.10667 to 0.08561, saving model to ./data/Data_LSTM1layer_100-38-0.0856.hdf5\n",
      "17652/17652 [==============================] - 33s 2ms/step - loss: 0.0856\n",
      "Epoch 39/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0639\n",
      "Epoch 00039: loss improved from 0.08561 to 0.06386, saving model to ./data/Data_LSTM1layer_100-39-0.0639.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0639\n",
      "Epoch 40/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0451\n",
      "Epoch 00040: loss improved from 0.06386 to 0.04507, saving model to ./data/Data_LSTM1layer_100-40-0.0451.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0451\n",
      "Epoch 41/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0260\n",
      "Epoch 00041: loss improved from 0.04507 to 0.02600, saving model to ./data/Data_LSTM1layer_100-41-0.0260.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0260\n",
      "Epoch 42/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0185\n",
      "Epoch 00042: loss improved from 0.02600 to 0.01852, saving model to ./data/Data_LSTM1layer_100-42-0.0185.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0185\n",
      "Epoch 43/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00043: loss improved from 0.01852 to 0.01667, saving model to ./data/Data_LSTM1layer_100-43-0.0167.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0167\n",
      "Epoch 44/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0133\n",
      "Epoch 00044: loss improved from 0.01667 to 0.01331, saving model to ./data/Data_LSTM1layer_100-44-0.0133.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0133\n",
      "Epoch 45/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0111\n",
      "Epoch 00045: loss improved from 0.01331 to 0.01115, saving model to ./data/Data_LSTM1layer_100-45-0.0111.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0111\n",
      "Epoch 46/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0097\n",
      "Epoch 00046: loss improved from 0.01115 to 0.00967, saving model to ./data/Data_LSTM1layer_100-46-0.0097.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0097\n",
      "Epoch 47/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0097\n",
      "Epoch 00047: loss improved from 0.00967 to 0.00966, saving model to ./data/Data_LSTM1layer_100-47-0.0097.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0097\n",
      "Epoch 48/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0081\n",
      "Epoch 00048: loss improved from 0.00966 to 0.00814, saving model to ./data/Data_LSTM1layer_100-48-0.0081.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0081\n",
      "Epoch 49/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0074\n",
      "Epoch 00049: loss improved from 0.00814 to 0.00742, saving model to ./data/Data_LSTM1layer_100-49-0.0074.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0074\n",
      "Epoch 50/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0080\n",
      "Epoch 00050: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0081\n",
      "Epoch 51/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.3920\n",
      "Epoch 00051: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.3975\n",
      "Epoch 52/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.8679\n",
      "Epoch 00052: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.8669\n",
      "Epoch 53/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.3845\n",
      "Epoch 00053: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.3841\n",
      "Epoch 54/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.1961\n",
      "Epoch 00054: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.1963\n",
      "Epoch 55/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0967\n",
      "Epoch 00055: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0965\n",
      "Epoch 56/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0540\n",
      "Epoch 00056: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0540\n",
      "Epoch 57/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0290\n",
      "Epoch 00057: loss did not improve\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 0.0289\n",
      "Epoch 58/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0192\n",
      "Epoch 00058: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0192\n",
      "Epoch 59/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0149\n",
      "Epoch 00059: loss did not improve\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 0.0149\n",
      "Epoch 60/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0124\n",
      "Epoch 00060: loss did not improve\n",
      "17652/17652 [==============================] - 27s 2ms/step - loss: 0.0124\n",
      "Epoch 61/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0107\n",
      "Epoch 00061: loss did not improve\n",
      "17652/17652 [==============================] - 27s 2ms/step - loss: 0.0107\n",
      "Epoch 62/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0094\n",
      "Epoch 00062: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0094\n",
      "Epoch 63/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0083\n",
      "Epoch 00063: loss did not improve\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 0.0083\n",
      "Epoch 64/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0075\n",
      "Epoch 00064: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0074\n",
      "Epoch 65/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0067\n",
      "Epoch 00065: loss improved from 0.00742 to 0.00668, saving model to ./data/Data_LSTM1layer_100-65-0.0067.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0067\n",
      "Epoch 66/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Epoch 00066: loss improved from 0.00668 to 0.00604, saving model to ./data/Data_LSTM1layer_100-66-0.0060.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0060\n",
      "Epoch 67/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0055\n",
      "Epoch 00067: loss improved from 0.00604 to 0.00548, saving model to ./data/Data_LSTM1layer_100-67-0.0055.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0050\n",
      "Epoch 00068: loss improved from 0.00548 to 0.00499, saving model to ./data/Data_LSTM1layer_100-68-0.0050.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0050\n",
      "Epoch 69/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0045\n",
      "Epoch 00069: loss improved from 0.00499 to 0.00454, saving model to ./data/Data_LSTM1layer_100-69-0.0045.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 0.0045\n",
      "Epoch 70/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0042\n",
      "Epoch 00070: loss improved from 0.00454 to 0.00415, saving model to ./data/Data_LSTM1layer_100-70-0.0042.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0042\n",
      "Epoch 71/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0038\n",
      "Epoch 00071: loss improved from 0.00415 to 0.00381, saving model to ./data/Data_LSTM1layer_100-71-0.0038.hdf5\n",
      "17652/17652 [==============================] - 28s 2ms/step - loss: 0.0038\n",
      "Epoch 72/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0035\n",
      "Epoch 00072: loss improved from 0.00381 to 0.00349, saving model to ./data/Data_LSTM1layer_100-72-0.0035.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0035\n",
      "Epoch 73/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0032\n",
      "Epoch 00073: loss improved from 0.00349 to 0.00321, saving model to ./data/Data_LSTM1layer_100-73-0.0032.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 0.0032\n",
      "Epoch 74/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0030\n",
      "Epoch 00074: loss improved from 0.00321 to 0.00296, saving model to ./data/Data_LSTM1layer_100-74-0.0030.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0030\n",
      "Epoch 75/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0027\n",
      "Epoch 00075: loss improved from 0.00296 to 0.00273, saving model to ./data/Data_LSTM1layer_100-75-0.0027.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0027\n",
      "Epoch 76/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0025\n",
      "Epoch 00076: loss improved from 0.00273 to 0.00251, saving model to ./data/Data_LSTM1layer_100-76-0.0025.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0025\n",
      "Epoch 77/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0023\n",
      "Epoch 00077: loss improved from 0.00251 to 0.00232, saving model to ./data/Data_LSTM1layer_100-77-0.0023.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0023\n",
      "Epoch 78/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00078: loss improved from 0.00232 to 0.00215, saving model to ./data/Data_LSTM1layer_100-78-0.0021.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0021\n",
      "Epoch 79/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00079: loss improved from 0.00215 to 0.00198, saving model to ./data/Data_LSTM1layer_100-79-0.0020.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0020\n",
      "Epoch 80/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00080: loss improved from 0.00198 to 0.00183, saving model to ./data/Data_LSTM1layer_100-80-0.0018.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0018\n",
      "Epoch 81/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00081: loss improved from 0.00183 to 0.00170, saving model to ./data/Data_LSTM1layer_100-81-0.0017.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0017\n",
      "Epoch 82/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00082: loss improved from 0.00170 to 0.00158, saving model to ./data/Data_LSTM1layer_100-82-0.0016.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0016\n",
      "Epoch 83/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.5223\n",
      "Epoch 00083: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.5296\n",
      "Epoch 84/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.0411\n",
      "Epoch 00084: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.0407\n",
      "Epoch 85/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.4582\n",
      "Epoch 00085: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.4583\n",
      "Epoch 86/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.2392\n",
      "Epoch 00086: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.2388\n",
      "Epoch 87/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.1159\n",
      "Epoch 00087: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.1160\n",
      "Epoch 88/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0623\n",
      "Epoch 00088: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0624\n",
      "Epoch 89/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0348\n",
      "Epoch 00089: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0348\n",
      "Epoch 90/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0204\n",
      "Epoch 00090: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0204\n",
      "Epoch 91/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0150\n",
      "Epoch 00091: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0150\n",
      "Epoch 92/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0122\n",
      "Epoch 00092: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0122\n",
      "Epoch 93/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0103\n",
      "Epoch 00093: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0103\n",
      "Epoch 94/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0089\n",
      "Epoch 00094: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0090\n",
      "Epoch 95/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 00095: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0079\n",
      "Epoch 96/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 00096: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0070\n",
      "Epoch 97/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0062\n",
      "Epoch 00097: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0062\n",
      "Epoch 98/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0056\n",
      "Epoch 00098: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0056\n",
      "Epoch 99/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0050\n",
      "Epoch 00099: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0050\n",
      "Epoch 100/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.0046\n",
      "Epoch 00100: loss did not improve\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 0.0046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x125e3a240>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "fname=\"./data/Data_LSTM1layer_100-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(fname, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(Xt, Yt, epochs=100, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate poems from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CharToInt(charint,text):\n",
    "    return np.array([charint[char] for char in text])\n",
    "    \n",
    "def IntToChar(intchar,text):\n",
    "    return \"\".join([intchar[char] for char in text])\n",
    "\n",
    "# helper function to sample an index from a probability array\n",
    "def sample(a, temperature=1.0):\n",
    "    A = np.log(a) / temperature\n",
    "    A = np.exp(A)\n",
    "    A = A/np.sum(A)*.99\n",
    "    return np.argmax(np.random.multinomial(1, A))\n",
    "\n",
    "# If char is letter in word list\n",
    "def IsWordLetter(char):\n",
    "    letters = '\\'abcdefghijklmnopqrstuvwxyz'\n",
    "    # If the current letter is in word list, then return 1\n",
    "    for i in letters:\n",
    "        if char==i:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def validWords(text,wordlist,charint,intchar):\n",
    "    # Valid letters in wordlist\n",
    "    letters = '\\'abcdefghijklmnopqrstuvwxyz'\n",
    "    strtext = IntToChar(intchar,text)\n",
    "    # If the current letter is not in word list, then all character options are valid\n",
    "    if IsWordLetter(strtext[-1])==False:\n",
    "        return 1\n",
    "    \n",
    "    # Find current word we are building\n",
    "    for i in range(1,200):\n",
    "        curword = strtext[len(strtext)-i:]\n",
    "        if IsWordLetter(curword[0])==False:\n",
    "            curword = curword[1:]\n",
    "            break\n",
    "    # Find if current word is in list of words\n",
    "    L = []\n",
    "    for word in wordlist:\n",
    "        x = word.find(curword)\n",
    "        if x==0:\n",
    "            if len(curword) < len(word):\n",
    "                L.append(word[len(curword)])\n",
    "\n",
    "    # If no words in dictionary are found then the word must end\n",
    "    if L==[]:\n",
    "        L = [' ','\\n',':',';']\n",
    "    L=np.unique(L)\n",
    "    # Build output vector\n",
    "    out = np.zeros(len(charint))\n",
    "    for i in L:\n",
    "        out[charint[i]]=1.0\n",
    "    return out\n",
    "\n",
    "def WordSample(text, wordlist, charint,intchar, a, temperature=1.0):\n",
    "    A = np.log(a) / temperature\n",
    "    A = np.exp(A)\n",
    "    b = validWords(text,wordlist,charint,intchar)\n",
    "    A = np.multiply(A,b)\n",
    "    # Normalize\n",
    "    A = A/np.sum(A)*.99\n",
    "    return np.argmax(np.random.multinomial(1, A))\n",
    "\n",
    "def generatePoem(model,intchar,charint,seed,temp=1.0):\n",
    "    print('Seed = ',seed)\n",
    "    IntSeed = CharToInt(charint,seed)\n",
    "    IntOut = IntSeed\n",
    "    temp = 1.0\n",
    "    lines = 13\n",
    "    # generate characters\n",
    "    for i in range(1000):\n",
    "        X = IntOut[i:i+Ntime]\n",
    "        OneHot_X = keras.utils.np_utils.to_categorical([X],num_classes=len(charint))\n",
    "        Ypred = model.predict(OneHot_X)\n",
    "        idx = sample(Ypred[0],temp)\n",
    "        IntOut = np.concatenate((IntOut,[idx]))\n",
    "        # Count number of poem lines generated\n",
    "        if idx==0:\n",
    "            lines-=1\n",
    "        if lines==0:\n",
    "            break\n",
    "    return IntToChar(intchar,IntOut)\n",
    "\n",
    "# Generate poem constraining to real words\n",
    "def generatePoem2(model,wordlist,intchar,charint,seed,temp=1.0):\n",
    "    print('Seed = ',seed)\n",
    "    IntSeed = CharToInt(charint,seed)\n",
    "    IntOut = IntSeed\n",
    "    temp = 1.0\n",
    "    lines = 13\n",
    "    # generate characters\n",
    "    for i in range(1000):\n",
    "        X = IntOut[i:i+Ntime]\n",
    "        OneHot_X = keras.utils.np_utils.to_categorical([X],num_classes=len(charint))\n",
    "        Ypred = model.predict(OneHot_X)\n",
    "        idx = WordSample(IntOut,wordlist,charint,intchar,Ypred[0],temp)\n",
    "        IntOut = np.concatenate((IntOut,[idx]))\n",
    "        # Count number of poem lines generated\n",
    "        if idx==0:\n",
    "            lines-=1\n",
    "        if lines==0:\n",
    "            break\n",
    "    return IntToChar(intchar,IntOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poem at temp =  1.5 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "the simf and day, and parsube a beermer;\n",
      "o grast me in waring tand wall if lowe,\n",
      "tho ghat the cemplaved white thy praine,\n",
      ":secings the herpsto d to thes to be tofleds be ther,\n",
      "whan ffor i love farrich you word's se fante,\n",
      "lice momh mps mut this my purt cearn thiughtz.\n",
      "  knowe in not self ar il besice to sing,\n",
      "alt yout to theme wornd so cuncesing theez\n",
      "them shaved arivery now,\n",
      "hau douth thes my nerezt and you altherzing treen\n",
      "cozess mijered she thy erpeode nce rhis hme,\n",
      "loking grouty cheilt lach oundw things my ming,\n",
      "whels offingrest sink oot as thy croult frown?\n",
      "\n",
      "Generated Poem at temp =  0.75 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "thet brouty bind you ast love spreppeace,\n",
      "but in to in so this dpariing llase (pork,\n",
      "why shough d ar in thy hade d rice genting?\n",
      "but no gake clliee notpiris my nempendd.\n",
      "s miner no hing, and byon thle nempead,\n",
      "whe chall dey ppire leckine eys bist knepss of my sreemoft daded.\n",
      "thy fornte azave''s sprace, andize hen fire,\n",
      "wilt blanty's vence, but dey by mand nr.\n",
      "  wrrthor thy remightlet may not ent reshith hid \n",
      "by pane me non wowt upound,\n",
      "the vor a coll, nree hos histir grain, orthit sum of my greckseach sig, my mened\n",
      ":ut the roth then and ere,\n",
      "and love's so douts thit thy waint thaight love i laak''s mind,\n",
      "\n",
      "Generated Poem at temp =  0.25 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "thes zifin and fail whith by wey sheet,\n",
      "then goves do in my prefode thy broudlds,\n",
      "d aalt should sover, whet beauty's best,\n",
      "fing the payst risccever i tryee ait,\n",
      "all blome but merife wouts,\n",
      "bhat th se fanger thee for my hos ar hail,\n",
      "farioun the rose whess the the wordd by ank,\n",
      "and love me anwermeruss dookndss,\n",
      "f rare her seaver, ozed having toote to ei.\n",
      "  with latune heart, th tr thy toustel'st food autherp taye,\n",
      "and that seauty appus wot a timeen softe,\n",
      "  thin ty that thou toul'zt kerpe me mek,\n",
      "war ill sinf azt an my beiffand pbride,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "temp = [1.5,0.75,0.25]\n",
    "for i in temp:\n",
    "    print('Generated Poem at temp = ',i,':')\n",
    "    print(generatePoem(model,intchar,charint,seed,temp=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poem at temp =  1.5 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "thyself sooner indeed thence surly and pierced:\n",
      "but merits outstripped tombed thence farther bearer:\n",
      "but thoughts forth thousand shz loves dost;\n",
      "matter thyself glowing loves my fell white faring:\n",
      "late history theez diseased painting lastingz:\n",
      "but thoughts beauty's sooner that's doom and loves former brainsz\n",
      "thence thanks wolf issueless ornaments;ignorance vilest reeks\n",
      "one interest thence tombed tombed thyself transgression surfeit hides\n",
      "wolf most folly semblance highmost coz\n",
      "touches thence took:\n",
      "  sorrows theirs imitated themes wz;celez thez both\n",
      "and that's borne telling to-themselves reeleth\n",
      "thee wert ere varying ornaments often owes\n",
      "\n",
      "Generated Poem at temp =  0.75 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "these bereft rather under told touches ornaments:\n",
      "fortune's songs;\n",
      "and titles often inconstant under something thence forlorn rights former says:\n",
      "overthrow my jealousy dimmed my head:\n",
      "thee horses roses bereft seals offenders;\n",
      "bettered touched therefore youth's ordering stars\n",
      ":nourished youth's stol'n anticipate muses:\n",
      "zealous words offenders bereft shallowest\n",
      "that's sz wolf urge aid they being my defaced\n",
      ".\n",
      "and doom nothing az now astonished sheaves:\n",
      "zealous beast seconds oftez swears hastenz languished double-finds\n",
      "orient my herd spread likeness knit\n",
      "\n",
      "Generated Poem at temp =  0.25 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "these bereft read age's thee inward often sighz\n",
      "thence strength's arts and ashes souls snow\n",
      "  whence ere pattern abundant others' journey:zealoz trees:\n",
      "  zealous looks offences dost zealous by thyself\n",
      "lov'st sheaves strengthened breathz wills\n",
      " and injurious air amends\n",
      "themes if nor fairer history herd;\n",
      "whence hindmost hot theirs it enclose\n",
      "windows after-summers':essays yours memory friend.\n",
      "lour'st once bestow'st towers outstripped;\n",
      "ashez doom halt theirs ignorance issueless:\n",
      "indeed thz down-these issueless deeds:\n",
      "thence horses wombs offenders often something:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "temp = [1.5,0.75,0.25]\n",
    "for i in temp:\n",
    "    print('Generated Poem at temp = ',i,':')\n",
    "    print(generatePoem2(model,wordlist,intchar,charint,seed,temp=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 40, 200)           191200    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 40, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 38)                7638      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 38)                0         \n",
      "=================================================================\n",
      "Total params: 519,638\n",
      "Trainable params: 519,638\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2 layer LSTM\n",
    "Nchars = len(charint)\n",
    "Ntime = 40\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(200, input_shape=(Xt.shape[1],Xt.shape[2]),return_sequences=True))\n",
    "model2.add(Dropout(0.0))\n",
    "model2.add(LSTM(200))\n",
    "model2.add(Dropout(0.0))\n",
    "model2.add(Dense(Nchars))\n",
    "model2.add(Activation('softmax'))\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 3.0238\n",
      "Epoch 00001: loss improved from inf to 3.02255, saving model to ./data/Data_LSTM2layer_100-01-3.0226.hdf5\n",
      "17652/17652 [==============================] - 64s 4ms/step - loss: 3.0226\n",
      "Epoch 2/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.7361\n",
      "Epoch 00002: loss improved from 3.02255 to 2.73458, saving model to ./data/Data_LSTM2layer_100-02-2.7346.hdf5\n",
      "17652/17652 [==============================] - 63s 4ms/step - loss: 2.7346\n",
      "Epoch 3/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.3768\n",
      "Epoch 00003: loss improved from 2.73458 to 2.37633, saving model to ./data/Data_LSTM2layer_100-03-2.3763.hdf5\n",
      "17652/17652 [==============================] - 63s 4ms/step - loss: 2.3763\n",
      "Epoch 4/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.2271\n",
      "Epoch 00004: loss improved from 2.37633 to 2.22731, saving model to ./data/Data_LSTM2layer_100-04-2.2273.hdf5\n",
      "17652/17652 [==============================] - 63s 4ms/step - loss: 2.2273\n",
      "Epoch 5/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.1144\n",
      "Epoch 00005: loss improved from 2.22731 to 2.11506, saving model to ./data/Data_LSTM2layer_100-05-2.1151.hdf5\n",
      "17652/17652 [==============================] - 63s 4ms/step - loss: 2.1151\n",
      "Epoch 6/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.0247\n",
      "Epoch 00006: loss improved from 2.11506 to 2.02493, saving model to ./data/Data_LSTM2layer_100-06-2.0249.hdf5\n",
      "17652/17652 [==============================] - 63s 4ms/step - loss: 2.0249\n",
      "Epoch 7/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.9428\n",
      "Epoch 00007: loss improved from 2.02493 to 1.94337, saving model to ./data/Data_LSTM2layer_100-07-1.9434.hdf5\n",
      "17652/17652 [==============================] - 63s 4ms/step - loss: 1.9434\n",
      "Epoch 8/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.8777\n",
      "Epoch 00008: loss improved from 1.94337 to 1.87680, saving model to ./data/Data_LSTM2layer_100-08-1.8768.hdf5\n",
      "17652/17652 [==============================] - 63s 4ms/step - loss: 1.8768\n",
      "Epoch 9/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.8150\n",
      "Epoch 00009: loss improved from 1.87680 to 1.81555, saving model to ./data/Data_LSTM2layer_100-09-1.8155.hdf5\n",
      "17652/17652 [==============================] - 72s 4ms/step - loss: 1.8155\n",
      "Epoch 10/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.7586\n",
      "Epoch 00010: loss improved from 1.81555 to 1.75826, saving model to ./data/Data_LSTM2layer_100-10-1.7583.hdf5\n",
      "17652/17652 [==============================] - 70s 4ms/step - loss: 1.7583\n",
      "Epoch 11/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.7002\n",
      "Epoch 00011: loss improved from 1.75826 to 1.69967, saving model to ./data/Data_LSTM2layer_100-11-1.6997.hdf5\n",
      "17652/17652 [==============================] - 71s 4ms/step - loss: 1.6997\n",
      "Epoch 12/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.6379\n",
      "Epoch 00012: loss improved from 1.69967 to 1.63752, saving model to ./data/Data_LSTM2layer_100-12-1.6375.hdf5\n",
      "17652/17652 [==============================] - 71s 4ms/step - loss: 1.6375\n",
      "Epoch 13/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.5775\n",
      "Epoch 00013: loss improved from 1.63752 to 1.57741, saving model to ./data/Data_LSTM2layer_100-13-1.5774.hdf5\n",
      "17652/17652 [==============================] - 63s 4ms/step - loss: 1.5774\n",
      "Epoch 14/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.5108\n",
      "Epoch 00014: loss improved from 1.57741 to 1.51107, saving model to ./data/Data_LSTM2layer_100-14-1.5111.hdf5\n",
      "17652/17652 [==============================] - 63s 4ms/step - loss: 1.5111\n",
      "Epoch 15/100\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.4379\n",
      "Epoch 00015: loss improved from 1.51107 to 1.43883, saving model to ./data/Data_LSTM2layer_100-15-1.4388.hdf5\n",
      "17652/17652 [==============================] - 63s 4ms/step - loss: 1.4388\n",
      "Epoch 16/100\n",
      "10880/17652 [=================>............] - ETA: 24s - loss: 1.3494"
     ]
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "fname=\"./data/Data_LSTM2layer_100-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(fname, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model2.fit(Xt, Yt, epochs=100, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "temp = [1.5,0.75,0.25]\n",
    "for i in temp:\n",
    "    print('Generated Poem at temp = ',i,':')\n",
    "    print(generatePoem(model2,intchar,charint,seed,temp=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "temp = [1.5,0.75,0.25]\n",
    "for i in temp:\n",
    "    print('Generated Poem at temp = ',i,':')\n",
    "    print(generatePoem2(model2,wordlist,intchar,charint,seed,temp=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
