{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM poem generation for Shakespeare's sonnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, ',': 6, '-': 7, '.': 8, ':': 9, ';': 10, '?': 11, 'a': 12, 'b': 13, 'c': 14, 'd': 15, 'e': 16, 'f': 17, 'g': 18, 'h': 19, 'i': 20, 'j': 21, 'k': 22, 'l': 23, 'm': 24, 'n': 25, 'o': 26, 'p': 27, 'q': 28, 'r': 29, 's': 30, 't': 31, 'u': 32, 'v': 33, 'w': 34, 'x': 35, 'y': 36, 'z': 37}\n"
     ]
    }
   ],
   "source": [
    "# Function to get Shakespeare's poems from file\n",
    "def getPoems():\n",
    "    with open(\"./data/shakespeare.txt\", \"r\") as f:\n",
    "        data = f.read().lower()\n",
    "    # Split by poems\n",
    "    poems = data.split(\"\\n\\n\\n\")\n",
    "    # Remove 1st line of each poem\n",
    "    out = []\n",
    "    for poem in poems:\n",
    "        for i in range(len(poem)):\n",
    "            if poem[i]=='\\n':\n",
    "                break\n",
    "        out.append(poem[i+1:])\n",
    "    return out\n",
    "\n",
    "# Get character to integer dictionary for one hot encoding\n",
    "def getChardict(poems):\n",
    "    # merge all poems and get list of characters\n",
    "    data = \"\".join(poems)\n",
    "    # Get dictionary of characters for one hot encoding\n",
    "    chars = sorted(list(set(data)))\n",
    "    charint = dict((c, i) for i, c in enumerate(chars))\n",
    "    intchar = dict((i, c) for i, c in enumerate(chars))\n",
    "    return charint,intchar\n",
    "\n",
    "# Integer encode the poems\n",
    "def getIntPoems(charint,poems):\n",
    "    out = []\n",
    "    for poem in poems:\n",
    "        out.append([charint[char] for char in poem])\n",
    "    return np.array(out)\n",
    "\n",
    "# Get array of poems\n",
    "poems = getPoems()\n",
    "# Get integer encoding dictionary\n",
    "charint,intchar = getChardict(poems)\n",
    "# Get Integer encoded poem array\n",
    "IntPoems = getIntPoems(charint,poems)\n",
    "\n",
    "print(charint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17735, 40, 48)\n",
      "(17735, 48)\n"
     ]
    }
   ],
   "source": [
    "# Generate X and Y training sets from each poem\n",
    "def getCharacters(poem,n=40,skip=10):\n",
    "    Xtrain = [poem[i:i+n] for i in range(0,len(poem)-n,skip)]\n",
    "    Ytrain = [poem[i+n] for i in range(0,len(poem)-n,skip)]\n",
    "    return Xtrain,Ytrain\n",
    "\n",
    "# Generate training data \n",
    "Ntime = 40\n",
    "skip = 5\n",
    "Xtrain = []\n",
    "Ytrain = []\n",
    "for poem in IntPoems:\n",
    "    Xt,Yt = getCharacters(poem,Ntime,skip)\n",
    "    Xtrain.append(Xt)\n",
    "    Ytrain.append(Yt)\n",
    "\n",
    "Ytrain = np.concatenate(Ytrain)\n",
    "Xtrain = np.concatenate(Xtrain)\n",
    "\n",
    "# One hot encode the training vectors\n",
    "import keras\n",
    "Yt = keras.utils.np_utils.to_categorical(Ytrain)\n",
    "Xt = keras.utils.np_utils.to_categorical(Xtrain)\n",
    "\n",
    "print(Xt.shape)\n",
    "print(Yt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 200)               199200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 48)                9648      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48)                0         \n",
      "=================================================================\n",
      "Total params: 208,848\n",
      "Trainable params: 208,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Dense, Activation, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "Nchars = len(charint)\n",
    "Ntime = 40\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, input_shape=(Xt.shape[1],Xt.shape[2])))\n",
    "model.add(Dropout(0.0))\n",
    "model.add(Dense(Nchars))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 3.0422\n",
      "Epoch 00001: loss improved from inf to 3.04097, saving model to ./data/Data_LSTM-01-3.0410.hdf5\n",
      "17735/17735 [==============================] - 31s 2ms/step - loss: 3.0410\n",
      "Epoch 2/20\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 2.7649\n",
      "Epoch 00002: loss improved from 3.04097 to 2.76346, saving model to ./data/Data_LSTM-02-2.7635.hdf5\n",
      "17735/17735 [==============================] - 31s 2ms/step - loss: 2.7635\n",
      "Epoch 3/20\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 2.4655\n",
      "Epoch 00003: loss improved from 2.76346 to 2.46589, saving model to ./data/Data_LSTM-03-2.4659.hdf5\n",
      "17735/17735 [==============================] - 29s 2ms/step - loss: 2.4659\n",
      "Epoch 4/20\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 2.3292\n",
      "Epoch 00004: loss improved from 2.46589 to 2.32910, saving model to ./data/Data_LSTM-04-2.3291.hdf5\n",
      "17735/17735 [==============================] - 27s 2ms/step - loss: 2.3291\n",
      "Epoch 5/20\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 2.2511\n",
      "Epoch 00005: loss improved from 2.32910 to 2.25187, saving model to ./data/Data_LSTM-05-2.2519.hdf5\n",
      "17735/17735 [==============================] - 27s 2ms/step - loss: 2.2519\n",
      "Epoch 6/20\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 2.1949\n",
      "Epoch 00006: loss improved from 2.25187 to 2.19591, saving model to ./data/Data_LSTM-06-2.1959.hdf5\n",
      "17735/17735 [==============================] - 27s 2ms/step - loss: 2.1959\n",
      "Epoch 7/20\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 2.1430\n",
      "Epoch 00007: loss improved from 2.19591 to 2.14301, saving model to ./data/Data_LSTM-07-2.1430.hdf5\n",
      "17735/17735 [==============================] - 27s 2ms/step - loss: 2.1430\n",
      "Epoch 8/20\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 2.1001\n",
      "Epoch 00008: loss improved from 2.14301 to 2.10054, saving model to ./data/Data_LSTM-08-2.1005.hdf5\n",
      "17735/17735 [==============================] - 27s 2ms/step - loss: 2.1005\n",
      "Epoch 9/20\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 2.0585\n",
      "Epoch 00009: loss improved from 2.10054 to 2.05907, saving model to ./data/Data_LSTM-09-2.0591.hdf5\n",
      "17735/17735 [==============================] - 27s 2ms/step - loss: 2.0591\n",
      "Epoch 10/20\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 2.0146\n",
      "Epoch 00010: loss improved from 2.05907 to 2.01609, saving model to ./data/Data_LSTM-10-2.0161.hdf5\n",
      "17735/17735 [==============================] - 27s 2ms/step - loss: 2.0161\n",
      "Epoch 11/20\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 1.9683\n",
      "Epoch 00011: loss improved from 2.01609 to 1.96859, saving model to ./data/Data_LSTM-11-1.9686.hdf5\n",
      "17735/17735 [==============================] - 27s 2ms/step - loss: 1.9686\n",
      "Epoch 12/20\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 1.9343\n",
      "Epoch 00012: loss improved from 1.96859 to 1.93480, saving model to ./data/Data_LSTM-12-1.9348.hdf5\n",
      "17735/17735 [==============================] - 27s 2ms/step - loss: 1.9348\n",
      "Epoch 13/20\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 1.9096\n",
      "Epoch 00013: loss improved from 1.93480 to 1.91033, saving model to ./data/Data_LSTM-13-1.9103.hdf5\n",
      "17735/17735 [==============================] - 27s 2ms/step - loss: 1.9103\n",
      "Epoch 14/20\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 1.8748\n",
      "Epoch 00014: loss improved from 1.91033 to 1.87391, saving model to ./data/Data_LSTM-14-1.8739.hdf5\n",
      "17735/17735 [==============================] - 27s 2ms/step - loss: 1.8739\n",
      "Epoch 15/20\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 1.8424\n",
      "Epoch 00015: loss improved from 1.87391 to 1.84188, saving model to ./data/Data_LSTM-15-1.8419.hdf5\n",
      "17735/17735 [==============================] - 26s 1ms/step - loss: 1.8419\n",
      "Epoch 16/20\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 1.8063\n",
      "Epoch 00016: loss improved from 1.84188 to 1.80757, saving model to ./data/Data_LSTM-16-1.8076.hdf5\n",
      "17735/17735 [==============================] - 26s 1ms/step - loss: 1.8076\n",
      "Epoch 17/20\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 1.7757\n",
      "Epoch 00017: loss improved from 1.80757 to 1.77621, saving model to ./data/Data_LSTM-17-1.7762.hdf5\n",
      "17735/17735 [==============================] - 26s 1ms/step - loss: 1.7762\n",
      "Epoch 18/20\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 1.7482\n",
      "Epoch 00018: loss improved from 1.77621 to 1.74822, saving model to ./data/Data_LSTM-18-1.7482.hdf5\n",
      "17735/17735 [==============================] - 26s 1ms/step - loss: 1.7482\n",
      "Epoch 19/20\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 1.7167\n",
      "Epoch 00019: loss improved from 1.74822 to 1.71823, saving model to ./data/Data_LSTM-19-1.7182.hdf5\n",
      "17735/17735 [==============================] - 26s 1ms/step - loss: 1.7182\n",
      "Epoch 20/20\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 1.6797\n",
      "Epoch 00020: loss improved from 1.71823 to 1.67987, saving model to ./data/Data_LSTM-20-1.6799.hdf5\n",
      "17735/17735 [==============================] - 26s 1ms/step - loss: 1.6799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11b472940>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "fname=\"./data/Data_LSTM-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(fname, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(Xt, Yt, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate poems from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CharToInt(charint,text):\n",
    "    return np.array([charint[char] for char in text])\n",
    "    \n",
    "def IntToChar(intchar,text):\n",
    "    return \"\".join([intchar[char] for char in text])\n",
    "\n",
    "# helper function to sample an index from a probability array\n",
    "def sample(a, temperature=1.0):\n",
    "    a = np.log(a) / temperature\n",
    "    a = np.exp(a)\n",
    "    a = a/np.sum(a)*.99\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))\n",
    "\n",
    "def generatePoem(model,intchar,charint,seed,temp=1.0):\n",
    "    print('Seed = ',seed)\n",
    "    IntSeed = CharToInt(charint,seed)\n",
    "    print('Integer Encoded Seed=',IntSeed)\n",
    "\n",
    "    IntOut = IntSeed\n",
    "    temp = 1.0\n",
    "    lines = 13\n",
    "    # generate characters\n",
    "    for i in range(1000):\n",
    "        X = IntOut[i:i+Ntime]\n",
    "        OneHot_X = keras.utils.np_utils.to_categorical([X],num_classes=len(charint))\n",
    "        Ypred = model.predict(OneHot_X)\n",
    "        idx = sample(Ypred[0],temp)\n",
    "        IntOut = np.concatenate((IntOut,[idx]))\n",
    "        # Count number of poem lines generated\n",
    "        if idx==0:\n",
    "            lines-=1\n",
    "        if lines==0:\n",
    "            break\n",
    "    return IntToChar(intchar,IntOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poem at temp =  1.5 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "Integer Encoded Seed= [40 29 22 33 33  1 30  1 24 36 34 37 22 39 26  1 41 29 26 26  1 41 36  1\n",
      " 22  1 40 42 34 34 26 39  3 40  1 25 22 46 21  0]\n",
      "shall i compare thee to a summer's day?\n",
      "and mas's hess arlofnturee io e-timessto gance,\n",
      "bact on with traughtress eracterat to mich.\n",
      "  ald toor owang in thy reaighs sa me, ary whire,\n",
      "or of 'ee iw the sofllvest frormew in il sond,\n",
      "theres storn thy  arveris wastworthinge in thenoumy\n",
      "i jearindzedes to thet, cell-erantu hire,\n",
      "than seef tebuty thean te be, agfoend tay so!\n",
      ",owet wele verise snombangsszened frate:\n",
      "afle searet thou hatr zagres aiten windaody.\n",
      "  bu liss tham in thou sow, moke to hith, heregzst?\n",
      "untheld yethy cornthings, tore ir my hzart beake blzert\n",
      " orethare withtheis im ereast, i gear the erweab, of tires roth ray me,\n",
      "tho wrirg bale on in ouths swarte,\n",
      "\n",
      "Generated Poem at temp =  0.75 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "Integer Encoded Seed= [40 29 22 33 33  1 30  1 24 36 34 37 22 39 26  1 41 29 26 26  1 41 36  1\n",
      " 22  1 40 42 34 34 26 39  3 40  1 25 22 46 21  0]\n",
      "shall i compare thee to a summer's day?\n",
      "pcaif your alover camanes thauth sore:\n",
      "hele braicisses farle so faor qiol ony my,\n",
      "verpnabutathe rimle sen lice and.y yree,z  lomzent, and1enowe ifzerzed\n",
      "mour thorezedrerestw)e stist of t?ceese.\n",
      "  thar ars een's mo mesprith my rame's arese,\n",
      " hat zeantountos of loss: in tout to thee,\n",
      "\n",
      " at me bituly stapiss fart is eeds ard aating wish,\n",
      "wring i it art having in thame lovesume dozs thou hepeet so bliss busela\n",
      "to bail.zaot coor end-e,\n",
      "thou lovesther'samy starn swelle, by conde,\n",
      "which frouget on carst ard andor thake theet-ofezserigst\n",
      "bry hapre wor with hatusce, at that main,zexone of hero thy piid,\n",
      "\n",
      "Generated Poem at temp =  0.25 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "Integer Encoded Seed= [40 29 22 33 33  1 30  1 24 36 34 37 22 39 26  1 41 29 26 26  1 41 36  1\n",
      " 22  1 40 42 34 34 26 39  3 40  1 25 22 46 21  0]\n",
      "shall i compare thee to a summer's day?\n",
      "whones remy but wigh to thy sifllightloce.\n",
      "fore waow, my lostes cored with my sear,\n",
      " and troush juy sall acaun hand growthis wiind:\n",
      "wnothor your watllce frowt tree trime,\n",
      "which har thainus your watbrelst bow sidor,\n",
      "and mare hon to beaite lokgerss blacmed.\n",
      " h tome now pritely, with thee atomerade,\n",
      "  ands and ou she lied these ale woath spot,\n",
      "to wie-wing my sheace mostowed grure bule.\n",
      "  be of treat rouns what tove misifremsorernf\n",
      "rome:\n",
      "dor the t)runks dzercongous to to juet conge,\n",
      "t anen this lakned, preypmering and or frote,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "temp = [1.5,0.75,0.25]\n",
    "for i in temp:\n",
    "    print('Generated Poem at temp = ',i,':')\n",
    "    print(generatePoem(model,intchar,charint,seed,temp=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 40, 200)           199200    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 40, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 48)                9648      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 48)                0         \n",
      "=================================================================\n",
      "Total params: 529,648\n",
      "Trainable params: 529,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2 layer LSTM\n",
    "Nchars = len(charint)\n",
    "Ntime = 40\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(200, input_shape=(Xt.shape[1],Xt.shape[2]),return_sequences=True))\n",
    "model2.add(Dropout(0.0))\n",
    "model2.add(LSTM(200))\n",
    "model2.add(Dropout(0.0))\n",
    "model2.add(Dense(Nchars))\n",
    "model2.add(Activation('softmax'))\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 3.0599\n",
      "Epoch 00001: loss improved from inf to 3.05924, saving model to ./data/Data_LSTM-01-3.0592.hdf5\n",
      "17735/17735 [==============================] - 76s 4ms/step - loss: 3.0592\n",
      "Epoch 2/20\n",
      "17024/17735 [===========================>..] - ETA: 2s - loss: 2.8270"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-02200c750098>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "fname=\"./data/Data_LSTM-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(fname, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model2.fit(Xt, Yt, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poem at temp =  1.5 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "Integer Encoded Seed= [40 29 22 33 33  1 30  1 24 36 34 37 22 39 26  1 41 29 26 26  1 41 36  1\n",
      " 22  1 40 42 34 34 26 39  3 40  1 25 22 46 21  0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "IntToChar() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9a6f758e0838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Generated Poem at temp = '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneratePoem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-df0d391c822f>\u001b[0m in \u001b[0;36mgeneratePoem\u001b[0;34m(model, seed, temp)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mIntToChar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIntOut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: IntToChar() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "temp = [1.5,0.75,0.25]\n",
    "for i in temp:\n",
    "    print('Generated Poem at temp = ',i,':')\n",
    "    print(generatePoem(model,seed,temp=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
