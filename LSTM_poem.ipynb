{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM poem generation for Shakespeare's sonnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get Shakespeare's poems from file\n",
    "def getPoems():\n",
    "    with open(\"./data/shakespeare.txt\", \"r\") as f:\n",
    "        data = f.read().lower()\n",
    "    # Split by poems\n",
    "    poems = data.split(\"\\n\\n\\n\")\n",
    "    # Remove 1st line of each poem\n",
    "    out = []\n",
    "    for poem in poems:\n",
    "        for i in range(len(poem)):\n",
    "            if poem[i]=='\\n':\n",
    "                break\n",
    "        out.append(poem[i+1:])\n",
    "    return out\n",
    "\n",
    "# Get character to integer dictionary for one hot encoding\n",
    "def getChardict(poems):\n",
    "    # merge all poems and get list of characters\n",
    "    data = \"\".join(poems)\n",
    "    # Get dictionary of characters for one hot encoding\n",
    "    chars = sorted(list(set(data)))\n",
    "    charint = dict((c, i) for i, c in enumerate(chars))\n",
    "    intchar = dict((i, c) for i, c in enumerate(chars))\n",
    "    return charint,intchar\n",
    "\n",
    "# Integer encode the poems\n",
    "def getIntPoems(charint,poems):\n",
    "    out = []\n",
    "    for poem in poems:\n",
    "        out.append([charint[char] for char in poem])\n",
    "    return np.array(out)\n",
    "\n",
    "# Get array of poems\n",
    "poems = getPoems()\n",
    "# Get integer encoding dictionary\n",
    "charint,intchar = getChardict(poems)\n",
    "# Get Integer encoded poem array\n",
    "IntPoems = getIntPoems(charint,poems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17652, 40, 38)\n",
      "(17652, 38)\n"
     ]
    }
   ],
   "source": [
    "# Generate X and Y training sets from each poem\n",
    "def getCharacters(poem,n=40,skip=10):\n",
    "    Xtrain = [poem[i:i+n] for i in range(0,len(poem)-n,skip)]\n",
    "    Ytrain = [poem[i+n] for i in range(0,len(poem)-n,skip)]\n",
    "    return Xtrain,Ytrain\n",
    "\n",
    "# Generate training data \n",
    "Ntime = 40\n",
    "skip = 5\n",
    "Xtrain = []\n",
    "Ytrain = []\n",
    "for poem in IntPoems:\n",
    "    Xt,Yt = getCharacters(poem,Ntime,skip)\n",
    "    Xtrain.append(Xt)\n",
    "    Ytrain.append(Yt)\n",
    "\n",
    "Ytrain = np.concatenate(Ytrain)\n",
    "Xtrain = np.concatenate(Xtrain)\n",
    "\n",
    "# One hot encode the training vectors\n",
    "import keras\n",
    "Yt = keras.utils.np_utils.to_categorical(Ytrain)\n",
    "Xt = keras.utils.np_utils.to_categorical(Xtrain)\n",
    "\n",
    "print(Xt.shape)\n",
    "print(Yt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 200)               191200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 38)                7638      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 38)                0         \n",
      "=================================================================\n",
      "Total params: 198,838\n",
      "Trainable params: 198,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Dense, Activation, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "Nchars = len(charint)\n",
    "Ntime = 40\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, input_shape=(Xt.shape[1],Xt.shape[2])))\n",
    "model.add(Dropout(0.0))\n",
    "model.add(Dense(Nchars))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 3.0025\n",
      "Epoch 00001: loss improved from inf to 3.00120, saving model to ./data/Data_LSTM-01-3.0012.hdf5\n",
      "17652/17652 [==============================] - 31s 2ms/step - loss: 3.0012\n",
      "Epoch 2/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.7078\n",
      "Epoch 00002: loss improved from 3.00120 to 2.70584, saving model to ./data/Data_LSTM-02-2.7058.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 2.7058\n",
      "Epoch 3/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.4079\n",
      "Epoch 00003: loss improved from 2.70584 to 2.40852, saving model to ./data/Data_LSTM-03-2.4085.hdf5\n",
      "17652/17652 [==============================] - 28s 2ms/step - loss: 2.4085\n",
      "Epoch 4/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.2846\n",
      "Epoch 00004: loss improved from 2.40852 to 2.28374, saving model to ./data/Data_LSTM-04-2.2837.hdf5\n",
      "17652/17652 [==============================] - 29s 2ms/step - loss: 2.2837\n",
      "Epoch 5/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.2007\n",
      "Epoch 00005: loss improved from 2.28374 to 2.20105, saving model to ./data/Data_LSTM-05-2.2011.hdf5\n",
      "17652/17652 [==============================] - 26s 1ms/step - loss: 2.2011\n",
      "Epoch 6/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.1369\n",
      "Epoch 00006: loss improved from 2.20105 to 2.13636, saving model to ./data/Data_LSTM-06-2.1364.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 2.1364\n",
      "Epoch 7/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.0844\n",
      "Epoch 00007: loss improved from 2.13636 to 2.08544, saving model to ./data/Data_LSTM-07-2.0854.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 2.0854\n",
      "Epoch 8/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.0361\n",
      "Epoch 00008: loss improved from 2.08544 to 2.03537, saving model to ./data/Data_LSTM-08-2.0354.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 2.0354\n",
      "Epoch 9/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.9885\n",
      "Epoch 00009: loss improved from 2.03537 to 1.98897, saving model to ./data/Data_LSTM-09-1.9890.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.9890\n",
      "Epoch 10/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.9401\n",
      "Epoch 00010: loss improved from 1.98897 to 1.94070, saving model to ./data/Data_LSTM-10-1.9407.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.9407\n",
      "Epoch 11/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.8902\n",
      "Epoch 00011: loss improved from 1.94070 to 1.89139, saving model to ./data/Data_LSTM-11-1.8914.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.8914\n",
      "Epoch 12/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.8495\n",
      "Epoch 00012: loss improved from 1.89139 to 1.84970, saving model to ./data/Data_LSTM-12-1.8497.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.8497\n",
      "Epoch 13/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.8065\n",
      "Epoch 00013: loss improved from 1.84970 to 1.80576, saving model to ./data/Data_LSTM-13-1.8058.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.8058\n",
      "Epoch 14/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.7670\n",
      "Epoch 00014: loss improved from 1.80576 to 1.76719, saving model to ./data/Data_LSTM-14-1.7672.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.7672\n",
      "Epoch 15/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.7272\n",
      "Epoch 00015: loss improved from 1.76719 to 1.72635, saving model to ./data/Data_LSTM-15-1.7264.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.7264\n",
      "Epoch 16/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.6819\n",
      "Epoch 00016: loss improved from 1.72635 to 1.68231, saving model to ./data/Data_LSTM-16-1.6823.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.6823\n",
      "Epoch 17/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.6396\n",
      "Epoch 00017: loss improved from 1.68231 to 1.64104, saving model to ./data/Data_LSTM-17-1.6410.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.6410\n",
      "Epoch 18/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.5905\n",
      "Epoch 00018: loss improved from 1.64104 to 1.59247, saving model to ./data/Data_LSTM-18-1.5925.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.5925\n",
      "Epoch 19/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.5462\n",
      "Epoch 00019: loss improved from 1.59247 to 1.54608, saving model to ./data/Data_LSTM-19-1.5461.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.5461\n",
      "Epoch 20/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.5118\n",
      "Epoch 00020: loss improved from 1.54608 to 1.51163, saving model to ./data/Data_LSTM-20-1.5116.hdf5\n",
      "17652/17652 [==============================] - 25s 1ms/step - loss: 1.5116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11ff01d68>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "fname=\"./data/Data_LSTM-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(fname, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(Xt, Yt, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate poems from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CharToInt(charint,text):\n",
    "    return np.array([charint[char] for char in text])\n",
    "    \n",
    "def IntToChar(intchar,text):\n",
    "    return \"\".join([intchar[char] for char in text])\n",
    "\n",
    "# helper function to sample an index from a probability array\n",
    "def sample(a, temperature=1.0):\n",
    "    a = np.log(a) / temperature\n",
    "    a = np.exp(a)\n",
    "    a = a/np.sum(a)*.99\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))\n",
    "\n",
    "def generatePoem(model,intchar,charint,seed,temp=1.0):\n",
    "    print('Seed = ',seed)\n",
    "    IntSeed = CharToInt(charint,seed)\n",
    "    IntOut = IntSeed\n",
    "    temp = 1.0\n",
    "    lines = 13\n",
    "    # generate characters\n",
    "    for i in range(1000):\n",
    "        X = IntOut[i:i+Ntime]\n",
    "        OneHot_X = keras.utils.np_utils.to_categorical([X],num_classes=len(charint))\n",
    "        Ypred = model.predict(OneHot_X)\n",
    "        idx = sample(Ypred[0],temp)\n",
    "        IntOut = np.concatenate((IntOut,[idx]))\n",
    "        # Count number of poem lines generated\n",
    "        if idx==0:\n",
    "            lines-=1\n",
    "        if lines==0:\n",
    "            break\n",
    "    return IntToChar(intchar,IntOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poem at temp =  1.5 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "d then os huve porsed adand youruli\n",
      "thou douchsung erefoous stove sumpered,\n",
      "and purse if thes beary thot helryss fird\n",
      "worhtulies douthes which thy eferthavened sfold,\n",
      "thut shosely or ad ave minesed exppacc;\n",
      "eats is buresmough disswide af with)\n",
      " ur in of mest a thisene,\n",
      "ow suckings thou asted sparned love's f?reg,,\n",
      "ouch if sous loffextr lave's priszeaped\n",
      "caill sum turseim cant:\n",
      "at ay's mo onewure ait thes,\n",
      "wsechell be and in theprad dyouch loven yoursert,\n",
      "to wenth thar herose more, now heperise,\n",
      "\n",
      "Generated Poem at temp =  0.75 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "the exporse of dory arine in mefer.\n",
      "so whar smine, and to you is ofy oungrenuds\n",
      "thou garsz af omy of sest love.\n",
      "  wrechoush thaucty have uppre, but ch artele,\n",
      "and thinghery andzet of free, bar ines pare,\n",
      "i cumfiin efemfbes cwikeey out not to rilt,\n",
      "more co mourerow dee you hich imyshe,\n",
      "r momenes de tois periouy dhight manes m mated.\n",
      "o and mo erethen s graice thon hase's from aferds arisig,\n",
      "whe ceme shee live, szeve, arve is in ollededt.\n",
      "that wery ulloin, whereshouted to dove dey,\n",
      "but in his more,\n",
      "on if in hes face whilld porse ound de seme,\n",
      "\n",
      "Generated Poem at temp =  0.25 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "shall i compare thee to a summer's day?\n",
      "weithe ant tounilerd pirnd on me thoughareihe and willd.\n",
      "ti cilfswert shy fall yout lake af nat,\n",
      "timn on truy zoro tre falmighe thou dass ablabe,\n",
      "the sores comases oby thy looses.\n",
      "and do sought if my semerthor swouth swemple do like?\n",
      "the willed praceckiet he hingere.\n",
      "the ho erese his haded of my beprize, pracke,\n",
      "that in nou whech in har light mish:\n",
      "whecend ie hame bo which utele not fiez,\n",
      "what no sweet shacl thed godstumy hith mwime:\n",
      "  thy soomed sewist love then meze kecned,\n",
      "net by urme be the zefred zid chemane.\n",
      "f ir tham i gicees beadstule zyed me blicbs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "temp = [1.5,0.75,0.25]\n",
    "for i in temp:\n",
    "    print('Generated Poem at temp = ',i,':')\n",
    "    print(generatePoem(model,intchar,charint,seed,temp=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 40, 200)           191200    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 40, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 38)                7638      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 38)                0         \n",
      "=================================================================\n",
      "Total params: 519,638\n",
      "Trainable params: 519,638\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2 layer LSTM\n",
    "Nchars = len(charint)\n",
    "Ntime = 40\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(200, input_shape=(Xt.shape[1],Xt.shape[2]),return_sequences=True))\n",
    "model2.add(Dropout(0.0))\n",
    "model2.add(LSTM(200))\n",
    "model2.add(Dropout(0.0))\n",
    "model2.add(Dense(Nchars))\n",
    "model2.add(Activation('softmax'))\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 3.0135\n",
      "Epoch 00001: loss improved from inf to 3.01231, saving model to ./data/Data_LSTM-01-3.0123.hdf5\n",
      "17652/17652 [==============================] - 67s 4ms/step - loss: 3.0123\n",
      "Epoch 2/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.6740\n",
      "Epoch 00002: loss improved from 3.01231 to 2.67342, saving model to ./data/Data_LSTM-02-2.6734.hdf5\n",
      "17652/17652 [==============================] - 64s 4ms/step - loss: 2.6734\n",
      "Epoch 3/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.3677\n",
      "Epoch 00003: loss improved from 2.67342 to 2.36747, saving model to ./data/Data_LSTM-03-2.3675.hdf5\n",
      "17652/17652 [==============================] - 64s 4ms/step - loss: 2.3675\n",
      "Epoch 4/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.2296\n",
      "Epoch 00004: loss improved from 2.36747 to 2.22862, saving model to ./data/Data_LSTM-04-2.2286.hdf5\n",
      "17652/17652 [==============================] - 63s 4ms/step - loss: 2.2286\n",
      "Epoch 5/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.1216\n",
      "Epoch 00005: loss improved from 2.22862 to 2.12061, saving model to ./data/Data_LSTM-05-2.1206.hdf5\n",
      "17652/17652 [==============================] - 63s 4ms/step - loss: 2.1206\n",
      "Epoch 6/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 2.0302\n",
      "Epoch 00006: loss improved from 2.12061 to 2.02923, saving model to ./data/Data_LSTM-06-2.0292.hdf5\n",
      "17652/17652 [==============================] - 63s 4ms/step - loss: 2.0292\n",
      "Epoch 7/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.9481\n",
      "Epoch 00007: loss improved from 2.02923 to 1.94754, saving model to ./data/Data_LSTM-07-1.9475.hdf5\n",
      "17652/17652 [==============================] - 64s 4ms/step - loss: 1.9475\n",
      "Epoch 8/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.8793\n",
      "Epoch 00008: loss improved from 1.94754 to 1.87937, saving model to ./data/Data_LSTM-08-1.8794.hdf5\n",
      "17652/17652 [==============================] - 68s 4ms/step - loss: 1.8794\n",
      "Epoch 9/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.8191\n",
      "Epoch 00009: loss improved from 1.87937 to 1.81830, saving model to ./data/Data_LSTM-09-1.8183.hdf5\n",
      "17652/17652 [==============================] - 71s 4ms/step - loss: 1.8183\n",
      "Epoch 10/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.7563\n",
      "Epoch 00010: loss improved from 1.81830 to 1.75636, saving model to ./data/Data_LSTM-10-1.7564.hdf5\n",
      "17652/17652 [==============================] - 75s 4ms/step - loss: 1.7564\n",
      "Epoch 11/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.6948\n",
      "Epoch 00011: loss improved from 1.75636 to 1.69521, saving model to ./data/Data_LSTM-11-1.6952.hdf5\n",
      "17652/17652 [==============================] - 65s 4ms/step - loss: 1.6952\n",
      "Epoch 12/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.6345\n",
      "Epoch 00012: loss improved from 1.69521 to 1.63513, saving model to ./data/Data_LSTM-12-1.6351.hdf5\n",
      "17652/17652 [==============================] - 63s 4ms/step - loss: 1.6351\n",
      "Epoch 13/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.5714\n",
      "Epoch 00013: loss improved from 1.63513 to 1.57030, saving model to ./data/Data_LSTM-13-1.5703.hdf5\n",
      "17652/17652 [==============================] - 63s 4ms/step - loss: 1.5703\n",
      "Epoch 14/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.4962\n",
      "Epoch 00014: loss improved from 1.57030 to 1.49695, saving model to ./data/Data_LSTM-14-1.4970.hdf5\n",
      "17652/17652 [==============================] - 64s 4ms/step - loss: 1.4970\n",
      "Epoch 15/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.4193\n",
      "Epoch 00015: loss improved from 1.49695 to 1.42019, saving model to ./data/Data_LSTM-15-1.4202.hdf5\n",
      "17652/17652 [==============================] - 64s 4ms/step - loss: 1.4202\n",
      "Epoch 16/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.3310\n",
      "Epoch 00016: loss improved from 1.42019 to 1.33181, saving model to ./data/Data_LSTM-16-1.3318.hdf5\n",
      "17652/17652 [==============================] - 64s 4ms/step - loss: 1.3318\n",
      "Epoch 17/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.2377\n",
      "Epoch 00017: loss improved from 1.33181 to 1.23758, saving model to ./data/Data_LSTM-17-1.2376.hdf5\n",
      "17652/17652 [==============================] - 68s 4ms/step - loss: 1.2376\n",
      "Epoch 18/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.1359\n",
      "Epoch 00018: loss improved from 1.23758 to 1.13686, saving model to ./data/Data_LSTM-18-1.1369.hdf5\n",
      "17652/17652 [==============================] - 77s 4ms/step - loss: 1.1369\n",
      "Epoch 19/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 1.0212\n",
      "Epoch 00019: loss improved from 1.13686 to 1.02261, saving model to ./data/Data_LSTM-19-1.0226.hdf5\n",
      "17652/17652 [==============================] - 82s 5ms/step - loss: 1.0226\n",
      "Epoch 20/20\n",
      "17536/17652 [============================>.] - ETA: 0s - loss: 0.9048\n",
      "Epoch 00020: loss improved from 1.02261 to 0.90565, saving model to ./data/Data_LSTM-20-0.9056.hdf5\n",
      "17652/17652 [==============================] - 93s 5ms/step - loss: 0.9056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12f953a90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "fname=\"./data/Data_LSTM-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(fname, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model2.fit(Xt, Yt, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poem at temp =  1.5 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "Integer Encoded Seed= [30 19 12 23 23  1 20  1 14 26 24 27 12 29 16  1 31 19 16 16  1 31 26  1\n",
      " 12  1 30 32 24 24 16 29  3 30  1 15 12 36 11  0]\n",
      "shall i compare thee to a summer's day?\n",
      "o  and sulflass marked not i gainty\n",
      "so blath to yzull are eich,\n",
      "and dur her illak, fare weros, and wilhy of with lav'st of zite,\n",
      "so lovethig then serind, and veruthes groienc:\n",
      "at the ublouklads time, fir than my lase,\n",
      " urush of this doein love praces zistign ounking,\n",
      "oo roughize's fie thought'szown thou touths when love?\n",
      " zench soml yrus dieple, our troughtdin fzemmy.\n",
      "  now right isznking though the anguened in a thy shedzedon\n",
      "to hil stoun sonotss sand,\n",
      "the wiends and averathou soughtull of thoued pnowife,\n",
      "that panpy, than pari him sowners stroncezllosk speng thoo sill-dich prrice.\n",
      "jightugarun outh croun, that ponene or thou hids,\n",
      "\n",
      "Generated Poem at temp =  0.75 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "Integer Encoded Seed= [30 19 12 23 23  1 20  1 14 26 24 27 12 29 16  1 31 19 16 16  1 31 26  1\n",
      " 12  1 30 32 24 24 16 29  3 30  1 15 12 36 11  0]\n",
      "shall i compare thee to a summer's day?\n",
      "izthe to me bearteririgh's (memen of er,\n",
      "but ounh hir wors wongh io hust in ti he?\n",
      " ruse cheamer shared no tot ere to for sreel.\n",
      "ho in no ground it tzee miming stoul sage,\n",
      "whace sond shat thy ellst be ir blouth hif thee sleng\n",
      "thom farle wan the bromperse,\n",
      "our in my fonty still bets brecsed,\n",
      "is heves i praist kind, nowzlis dot thee sprauseszokp?o the huth haith inzere, and swbelt,\n",
      "thowero suce chound this kenow, hat rombur lighed with likk.\n",
      "why st.els of this sill as eremy hep,rate,\n",
      "lowe' tz timle but zeme hou tillavaig the,\n",
      "wremene ey loth priod.\n",
      "anthe broune, and dsingsed,'s chime, thzings in loupe fins:\n",
      "\n",
      "Generated Poem at temp =  0.25 :\n",
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "Integer Encoded Seed= [30 19 12 23 23  1 20  1 14 26 24 27 12 29 16  1 31 19 16 16  1 31 26  1\n",
      " 12  1 30 32 24 24 16 29  3 30  1 15 12 36 11  0]\n",
      "shall i compare thee to a summer's day?\n",
      "in hi hereno ory fied, a fot no besuddld simez.\n",
      "a this sumerane uubl nif hoo best buth suldoond shall,\n",
      "that i him far ily freet:s andzesispars\n",
      "with thee no mine thou zictul is thy pads,\n",
      "and shagkt i betot welise, and time urceams,\n",
      " zrane comethe time's but thou grazes but foofsty:\n",
      "that then whinghe abl ther ngwers fay wooll-cleess thone,\n",
      "melon in yej my live the swest sull oke,\n",
      "wuchound yeu heart, winct thou sich lives ntinget thou waptzesture,\n",
      "thou lave ghich doot doscof las in gatker\n",
      "shi. thzel ruth i juguner th beat.\n",
      "to suce jincwadrest domy desill,\n",
      "  thene rownes ;r aince if my refof,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "temp = [1.5,0.75,0.25]\n",
    "for i in temp:\n",
    "    print('Generated Poem at temp = ',i,':')\n",
    "    print(generatePoem(model,intchar,charint,seed,temp=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
