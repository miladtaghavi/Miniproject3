{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM poem generation for Shakespeare's sonnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get Shakespeare's poems from file\n",
    "def getPoems():\n",
    "    with open(\"./data/shakespeare.txt\", \"r\") as f:\n",
    "        data = f.read().lower()\n",
    "    # Split by poems\n",
    "    poems = data.split(\"\\n\\n\\n\")\n",
    "    # Remove 1st line of each poem\n",
    "    poems = [poem[20:] for poem in poems]\n",
    "    return poems\n",
    "\n",
    "# Get character to integer dictionary for one hot encoding\n",
    "def getChardict(poems):\n",
    "    # merge all poems and get list of characters\n",
    "    data = \"\".join(poems)\n",
    "    # Get dictionary of characters for one hot encoding\n",
    "    chars = sorted(list(set(data)))\n",
    "    charint = dict((c, i) for i, c in enumerate(chars))\n",
    "    intchar = dict((i, c) for i, c in enumerate(chars))\n",
    "    return charint,intchar\n",
    "\n",
    "# Integer encode the poems\n",
    "def getIntPoems(charint,poems):\n",
    "    out = []\n",
    "    for poem in poems:\n",
    "        out.append([charint[char] for char in poem])\n",
    "    return np.array(out)\n",
    "\n",
    "# Get array of poems\n",
    "poems = getPoems()\n",
    "# Get integer encoding dictionary\n",
    "charint,intchar = getChardict(poems)\n",
    "# Get Integer encoded poem array\n",
    "IntPoems = getIntPoems(charint,poems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17735, 40, 48)\n",
      "(17735, 48)\n"
     ]
    }
   ],
   "source": [
    "# Generate X and Y training sets from each poem\n",
    "def getCharacters(poem,n=40,skip=10):\n",
    "    Xtrain = [poem[i:i+n] for i in range(0,len(poem)-n,skip)]\n",
    "    Ytrain = [poem[i+n] for i in range(0,len(poem)-n,skip)]\n",
    "    return Xtrain,Ytrain\n",
    "\n",
    "# Generate training data \n",
    "Ntime = 40\n",
    "skip = 5\n",
    "Xtrain = []\n",
    "Ytrain = []\n",
    "for poem in IntPoems:\n",
    "    Xt,Yt = getCharacters(poem,n,skip)\n",
    "    Xtrain.append(Xt)\n",
    "    Ytrain.append(Yt)\n",
    "\n",
    "Ytrain = np.concatenate(Ytrain)\n",
    "Xtrain = np.concatenate(Xtrain)\n",
    "\n",
    "# One hot encode the training vectors\n",
    "import keras\n",
    "Yt = keras.utils.np_utils.to_categorical(Ytrain)\n",
    "Xt = keras.utils.np_utils.to_categorical(Xtrain)\n",
    "\n",
    "print(Xt.shape)\n",
    "print(Yt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_15 (LSTM)               (None, 200)               199200    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 48)                9648      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 48)                0         \n",
      "=================================================================\n",
      "Total params: 208,848\n",
      "Trainable params: 208,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Dense, Activation, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "Nchars = len(charint)\n",
    "Ntime = 40\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, input_shape=(Xt.shape[1],Xt.shape[2]),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Nchars))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 3.0678\n",
      "Epoch 00001: loss improved from inf to 3.06760, saving model to ./data/Data_LSTM-01-3.0676.hdf5\n",
      "17735/17735 [==============================] - 32s 2ms/step - loss: 3.0676\n",
      "Epoch 2/10\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 2.8428\n",
      "Epoch 00002: loss improved from 3.06760 to 2.84196, saving model to ./data/Data_LSTM-02-2.8420.hdf5\n",
      "17735/17735 [==============================] - 29s 2ms/step - loss: 2.8420\n",
      "Epoch 3/10\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 2.5142\n",
      "Epoch 00003: loss improved from 2.84196 to 2.51404, saving model to ./data/Data_LSTM-03-2.5140.hdf5\n",
      "17735/17735 [==============================] - 30s 2ms/step - loss: 2.5140\n",
      "Epoch 4/10\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 2.3697\n",
      "Epoch 00004: loss improved from 2.51404 to 2.36951, saving model to ./data/Data_LSTM-04-2.3695.hdf5\n",
      "17735/17735 [==============================] - 26s 1ms/step - loss: 2.3695\n",
      "Epoch 5/10\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 2.2892\n",
      "Epoch 00005: loss improved from 2.36951 to 2.28834, saving model to ./data/Data_LSTM-05-2.2883.hdf5\n",
      "17735/17735 [==============================] - 30s 2ms/step - loss: 2.2883\n",
      "Epoch 6/10\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 2.2299\n",
      "Epoch 00006: loss improved from 2.28834 to 2.23018, saving model to ./data/Data_LSTM-06-2.2302.hdf5\n",
      "17735/17735 [==============================] - 29s 2ms/step - loss: 2.2302\n",
      "Epoch 7/10\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 2.1739\n",
      "Epoch 00007: loss improved from 2.23018 to 2.17389, saving model to ./data/Data_LSTM-07-2.1739.hdf5\n",
      "17735/17735 [==============================] - 30s 2ms/step - loss: 2.1739\n",
      "Epoch 8/10\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 2.1316\n",
      "Epoch 00008: loss improved from 2.17389 to 2.13131, saving model to ./data/Data_LSTM-08-2.1313.hdf5\n",
      "17735/17735 [==============================] - 30s 2ms/step - loss: 2.1313\n",
      "Epoch 9/10\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 2.0877\n",
      "Epoch 00009: loss improved from 2.13131 to 2.08813, saving model to ./data/Data_LSTM-09-2.0881.hdf5\n",
      "17735/17735 [==============================] - 29s 2ms/step - loss: 2.0881\n",
      "Epoch 10/10\n",
      "17664/17735 [============================>.] - ETA: 0s - loss: 2.0484\n",
      "Epoch 00010: loss improved from 2.08813 to 2.04856, saving model to ./data/Data_LSTM-10-2.0486.hdf5\n",
      "17735/17735 [==============================] - 27s 2ms/step - loss: 2.0486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1249da438>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "fname=\"./data/Data_LSTM-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(fname, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(Xt, Yt, epochs=10, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate poems from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed =  shall i compare thee to a summer's day?\n",
      "\n",
      "Integer Encoded Seed= [40 29 22 33 33  1 30  1 24 36 34 37 22 39 26  1 41 29 26 26  1 41 36  1\n",
      " 22  1 40 42 34 34 26 39  3 40  1 25 22 46 21  0]\n"
     ]
    }
   ],
   "source": [
    "def CharToInt(charint,text):\n",
    "    return np.array([charint[char] for char in text])\n",
    "    \n",
    "def IntToChar(intchar,text):\n",
    "    return \"\".join([intchar[char] for char in text])\n",
    "\n",
    "# helper function to sample an index from a probability array\n",
    "def sample(a, temperature=1.0):\n",
    "    a = np.log(a) / temperature\n",
    "    a = np.exp(a) / np.sum(np.exp(a))\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))\n",
    "\n",
    "# Seed the first line\n",
    "seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "print('Seed = ',seed)\n",
    "IntSeed = CharToInt(charint,seed)\n",
    "print('Integer Encoded Seed=',IntSeed)\n",
    "\n",
    "IntOut = IntSeed\n",
    "temp = 1.0\n",
    "lines = 13\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    X = IntOut[i:i+Ntime]\n",
    "    OneHot_X = keras.utils.np_utils.to_categorical([X],num_classes=len(charint))\n",
    "    Ypred = model.predict(OneHot_X)\n",
    "    idx = sample(Ypred[0],temp)\n",
    "    IntOut = np.concatenate((IntOut,[idx]))\n",
    "    # Count number of poem lines generated\n",
    "    if idx==0:\n",
    "        lines-=1\n",
    "    if lines==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "i by tor mm te ound she,\n",
      "thou d corve shehore wheh thest farmurt gali's,\n",
      "avem, on then thou tho love the htroy thou hur yean iw bein:y eie foren, and at the hatees'sum efingmeeddghets,\n",
      "whone ttay thee mave the listhr chabse chave hy al:\n",
      "a lout,\n",
      " eer ymer,\n",
      " have ibet ould thy ampey see dotgess bowel coris, ofauss nneredt drepriching mlrs'sung,\n",
      "ay hats teec aawl\n",
      "nomen dom hike tent ald damenouvent arleverds dilarw than awer alouy tromfroll my remand.\n",
      "d eeling thiuu paranet thau hove.\n",
      " or thecy whatht tous frale g ay sumanyes ferrmy demenes dathis stoun, hargr wo hape in in eroudher core butle;\n",
      "nhar cfourpyinge,\n",
      "aut andes beade theats presigks 'sem tore,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('')\n",
    "print(IntToChar(intchar,IntOut))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
